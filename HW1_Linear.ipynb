{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lycera/skribblbank/blob/main/HW1_Linear.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4oKbqMWMpMN"
      },
      "source": [
        "# CS1470/2470 HW1: Single-Layered Neural Networks\n",
        "\n",
        "In this homework assignment, you will build a simple linear model using differential modules.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CLyvlysHCK9"
      },
      "source": [
        "## Starting Our Modular API\n",
        "\n",
        "### **The goal of this assignment is as follows:** \n",
        "- Organize our understanding of deep learning into a modular framework. \n",
        "- Get familiarized with a simple modular API which reflects (but is a simplification of) some of PyTorch's systems. \n",
        "- Implement some nice modular components and be able to construct a functional single-file neural network from it. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFUeG0msHRyo"
      },
      "source": [
        "### Improving The Old Way via Chain Rule\n",
        "\n",
        "Recall that in the machine learning lab, we got the chance to make some simple but effective regression models! Given some realizations $X \\sim \\mathcal{X}$ and $Y = \\mathbb{E}[Y|X] + \\xi$, we were able to train up a model $h_{\\theta} \\in \\mathcal{H}$ which was similar to $\\mathbb{E}[Y|X]$ and thereby minimized an empirical loss $\\mathcal{L}$ of our choice.\n",
        "\n",
        "> As a reminder, $\\mathcal{X}$ is the space of all candidate functions.\n",
        "\n",
        "In these cases, we assumed that $h_{\\theta}$ had a relatively simple and non-flexible architecture, which meant that we could manually specify the structure and simply derive and code up our gradient formula once. Furthermore, since the optimization process was concave, we could even skip the gradient computation and directly derive a loss-minimizing parameter selection. \n",
        "\n",
        "Of course, there are hard limits to what this kind of architecture can provide us; sometimes the relationships that the model needs to capture are relatively complex and might not be resolvable in such a fashion. And that's why this course exists!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmnuKJmCHmpg"
      },
      "source": [
        "#### **Problem:** \n",
        "> This is extremely time-consuming and rigid! What happens if we switched out an activation function? A loss function? We'd have to re-specify the gradient every time!\n",
        "\n",
        "#### **Solution:** \n",
        "> Let's take advantage of the chain rule! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecdHbe0xHb1o"
      },
      "source": [
        "**Naive Solution:** If we really wanted to, we could approach this problem in the same way as before, and just code up the gradient functions manually. Similarly to before, this would allow us to propagate gradients through, say, a specified loss function, an activation function, and a dense layer. We could also do it for 2 dense layers; just use the old gradient function for the weights in layer 2, compute the new gradient for the weights in layer 1, and so on. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTfm_9fmHkhE"
      },
      "source": [
        "Recall that per the chain rule, if there exists a set of differentiable functions $c(b)$ and $b(a)$, then \n",
        "\n",
        "$$\\frac{\\partial a}{\\partial c} = \\frac{\\partial a}{\\partial b} \\frac{\\partial b}{\\partial c}$$\n",
        "\n",
        "Going back to our regression model, let's assume that we have a layered process: \n",
        "\n",
        "$$x \\to h_\\theta(x) \\to \\mathcal{L}(h_\\theta)$$\n",
        "\n",
        "This implies that we can compute the partial of the trainable parameters $\\theta$ through a loss evaluation $\\mathcal{L}$ and a dense layer $h_\\theta(x)$ by the following relationship:\n",
        "\n",
        "$$\\frac{\\partial \\mathcal{L}}{\\partial \\theta} = \\frac{\\partial \\mathcal{h_\\theta}}{\\partial \\theta}\\frac{\\partial \\mathcal{L}}{\\partial h_\\theta}$$\n",
        "\n",
        "With a similar logic, you can also make the assertion that you can also get the partial with respect to the input $x$:\n",
        "\n",
        "$$\\frac{\\partial \\mathcal{L}}{\\partial x} = \\frac{\\partial h_\\theta}{\\partial x}\\frac{\\partial \\mathcal{L}}{\\partial h_\\theta}$$\n",
        "\n",
        "So... by the same token, is there anything stopping us from going further? Let's say that we decided to have another hypothesis function such that $x = h'_{\\theta'}(x')$ for some other hypothesis function and inputs? The new structure would then be: \n",
        "\n",
        "$$x' \\to \\big[ x = h'_{\\theta'}(x') \\big] \\to h_\\theta(x) \\to \\mathcal{L}(h_\\theta)$$\n",
        "\n",
        "Without the chain rule, coding in the facilities to optimize $\\theta'$ might have been tricky, but with the chain rule we know that:\n",
        "\n",
        "$$\\frac{\\partial \\mathcal{L}}{\\partial \\theta'} \n",
        "= \\frac{\\partial x}{\\partial \\theta'}\\frac{\\partial h}{\\partial x}\\frac{\\partial \\mathcal{L}}{\\partial h} \n",
        "= \\frac{\\partial x}{\\partial \\theta'}\\frac{\\partial \\mathcal{L}}{\\partial x}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUJfaOFpIJbD"
      },
      "source": [
        "Notice how this process is both predictable and scales very well! Say that we wanted to add some activation functions to restrict the range of the hypothesis functions. This trivially inserts into the chain and everything still works and will look something like this: \n",
        "\n",
        "$$ \\frac{\\partial \\mathcal{L}}{\\partial x} = \\frac{\\partial h}{\\partial x}\\frac{\\partial a}{\\partial h}\\frac{\\partial \\mathcal{L}}{\\partial a} \n",
        "\\ \\text{ and } \\ \\frac{\\partial \\mathcal{L}}{\\partial \\theta} = \\frac{\\partial h}{\\partial \\theta}\\frac{\\partial a}{\\partial h}\\frac{\\partial \\mathcal{L}}{\\partial a} \n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVyTXT5TIOsz"
      },
      "source": [
        "And with that, we start to approach the reason why this is such a powerful formulation: The cumulative nature of the process. Specifically, consider the process that needs to happen in order to compute this for the extended 2-layer example: \n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\frac{\\partial a}{\\partial h}\\frac{\\partial \\mathcal{L}}{\\partial a} \n",
        "= \\frac{\\partial \\mathcal{L}}{\\partial h} \n",
        "&\\to \\cdots = \\frac{\\partial \\mathcal{L}}{\\partial x} \n",
        "&\\to \\cdots = \\frac{\\partial \\mathcal{L}}{\\partial a'}\n",
        "&\\to \\cdots = \\frac{\\partial \\mathcal{L}}{\\partial h'} \n",
        "&\\to \\cdots = \\frac{\\partial \\mathcal{L}}{\\partial x'} \n",
        "\\\\\n",
        "&\\searrow \\cdots = \\frac{\\partial \\mathcal{L}}{\\partial \\theta} \n",
        "&&&\\searrow \\cdots = \\frac{\\partial \\mathcal{L}}{\\partial \\theta'} \n",
        "% \\\\\n",
        "% &\\searrow \\cdots = \\frac{\\partial \\mathcal{L}}{\\partial \\theta} \n",
        "% \\to \\cdots = \\frac{\\partial \\mathcal{L}}{\\partial a'} \n",
        "% \\to \\cdots = \\frac{\\partial \\mathcal{L}}{\\partial h'} \n",
        "% &\n",
        "% \\to \\cdots = \\frac{\\partial \\mathcal{L}}{\\partial \\theta'} \n",
        "% \\\\ \n",
        "% &&\n",
        "% \\to \\cdots = \\frac{\\partial \\mathcal{L}}{\\partial x'} \n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "... and this is the process known as **back-propagation** *(and a special case of **auto-differentiation**)*!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdK16iXOVgta"
      },
      "source": [
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhgMs3u_S1OU"
      },
      "source": [
        "## Loading In Our Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tV7vjVbTf9Xg"
      },
      "source": [
        "The first thing we have to do is load in our data to use our model with. We'll be working with the [diabetes dataset from the sklearn package](https://scikit-learn.org/stable/datasets/toy_dataset.html#diabetes-dataset). We've already provided the code to load in the input data and the ground truth labels (stored as `X` and `Y` respectively below).\n",
        "\n",
        "**[TODO]:** Split the samples into training and testing sets. We'll train with the train set and reserve the testing set to evaluate the model's performance on samples it hasn't seen.\n",
        "\n",
        "The diabetes dataset has 442 samples. Each sample's input data has 10 data points for some key metrics, like age and cholesterol levels. The \"label\" is a number representing disease progression one year after baseline. Thus, `X` has shape `(442, 10)`, while `Y` has shape `(442,)`.\n",
        "\n",
        "**[TODO]:** Reshape the `Y` subsets to have shape `(num_samples, 1)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gytrDuxV5gFO",
        "outputId": "e0342569-3290-4c06-d86a-6783c6d652ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> Input shape: (353, 10) for training, (89, 10) for testing\n",
            "> Label shape: (353, 1) for training, (89, 1) for testing\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "diabetes = load_diabetes()\n",
        "X, Y = diabetes.data, diabetes.target\n",
        "\n",
        "## TODO: Split the data into a 80%-20% training-testing split\n",
        "## TODO: Reshape the Y subsets to have shape (num_samples, 1)\n",
        "X0, X1, Y0, Y1 = train_test_split(X, Y, train_size=0.8)\n",
        "\n",
        "Y0 = np.reshape(Y0, (Y0.shape[0], 1))\n",
        "Y1 = np.reshape(Y1, (Y1.shape[0], 1))\n",
        "\n",
        "print(f\"\"\"\n",
        "> Input shape: {X0.shape} for training, {X1.shape} for testing\n",
        "> Label shape: {Y0.shape} for training, {Y1.shape} for testing\n",
        "\"\"\".strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIYU_639VgHq"
      },
      "source": [
        "## **Exploring a possible modular implementation: PyTorch**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHs57_36jfDL"
      },
      "source": [
        "Next, we'll want to build up a Regression model interface, from which we can implement specific regression model classes, like the LinearRegression class we'll work with later. \n",
        "\n",
        "We subclass the `nn.Module class`, which represents any module (like a layer or model) of a deep learning system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DG6vkwv5chPg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Regression(nn.Module):\n",
        "\n",
        "    \"\"\"\n",
        "    Initialize all the inherent \"things\" inside of a model!\n",
        "    This includes things like the layers, activation/loss functions, and optimzer. \n",
        "    \"\"\"\n",
        "    def __init__(self, input_dims, output_dims):\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")        \n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(input_dims, output_dims).to(self.device)   \n",
        "        self.activation = None  ## To be specified in subclasses \n",
        "        self.loss = None        ## To be specified in subclasses \n",
        "        self.set_learning_rate()\n",
        "\n",
        "    \"\"\"\n",
        "    Sets up the optmizer\n",
        "    \"\"\"\n",
        "    def set_learning_rate(self, learning_rate=0.001):\n",
        "        self.optimizer = torch.optim.SGD(self.parameters(), lr=learning_rate) ## Simple stochastic gradient descent (SGD) optimizer\n",
        "\n",
        "    \"\"\"\n",
        "    Forward pass of the model\n",
        "    Given an input x, how does the model process the input to get its output?\n",
        "    \"\"\"\n",
        "    def forward(self, x):\n",
        "        x = self.dense(x)\n",
        "        x = self.activation(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTzEFlA0TFZB"
      },
      "source": [
        "## Adding a PyTorch Training/Evaluation Routine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBSK640PoDWX"
      },
      "source": [
        "Now that we have the basis for a Regression model, we need to describe how to train (`fit`) and evaluate (`evaluate`) our model, given data. \n",
        "Every epoch, we want to fit our model to the training data, and then evaluate our model on the testing data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "aUetVEGtS9k9"
      },
      "outputs": [],
      "source": [
        "class TrainTest:\n",
        "\n",
        "    no_grad = torch.no_grad\n",
        "\n",
        "    def fit(self, data):\n",
        "        ## Training loop\n",
        "        self.train()        ## Set model into training mode\n",
        "        ## Iterate over the data batches\n",
        "        for batch, (inputs, target) in enumerate(data):\n",
        "            ## In real pytorch, you'd need to set the device\n",
        "            inputs = inputs.to(self.device)\n",
        "            target = target.to(self.device)\n",
        "            ## Erase the gradient history\n",
        "            self.optimizer.zero_grad()\n",
        "            ## Do a forward pass on the model\n",
        "            output = self(inputs)\n",
        "            ## Compute the loss\n",
        "            loss = self.loss(output, target)\n",
        "            ## Run backwards pass from the loss through the previous layers\n",
        "            ## This will accumulate gradients for the parameters that need to be optimized\n",
        "            loss.backward()\n",
        "            ## Perform a single optimization step\n",
        "            self.optimizer.step()\n",
        "        return {'loss' : loss}\n",
        "\n",
        "    def evaluate(self, data):\n",
        "        ## Set model into \"evaluate\" mode so that the parameters don't get updated\n",
        "        self.eval()\n",
        "        total_loss = 0\n",
        "        ## Cut off the tensor training scope to make sure weights aren't updated\n",
        "        ## For now, it's torch.no_grad; later, you'll use Tensor.no_grad\n",
        "        with TrainTest.no_grad():\n",
        "            for inputs, target in data:\n",
        "                ## In real pytorch, you'd need to set the device\n",
        "                inputs = inputs.to(self.device)\n",
        "                target = target.to(self.device)\n",
        "                output = self(inputs)\n",
        "                total_loss += self.loss(output, target).item()  # sum up batch loss\n",
        "\n",
        "        total_loss /= len(data)\n",
        "        return {'test_loss' : total_loss}\n",
        "        \n",
        "    def train_test(self, train_data, test_data, epochs=1):\n",
        "        ## Does both training and validation on a per-epoch basis\n",
        "        all_stats = []\n",
        "        for epoch in range(epochs):\n",
        "            train_stats = self.fit(train_data)\n",
        "            test_stats = self.evaluate(test_data)\n",
        "            all_stats += [{**train_stats, **test_stats}]\n",
        "            print(f'[Epoch {epoch+1}/{epochs}]', all_stats[-1])\n",
        "        return all_stats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDNhzYnUTNMa"
      },
      "source": [
        "## Making Linear Regression with Train/Test Capabilities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njv7q2UTTgQE"
      },
      "source": [
        "Next, we implement the `LinearRegression` class, which subclasses both `Regression` and `TrainTest` to inherit useful methods. \n",
        "\n",
        "Consider a dense layer represented by $Y = XA + B$ where: \n",
        "- $X$ has shape `(n,input_dims)`\n",
        "- $A$ has shape `(input_dims,output_dims)`\n",
        "- $B$ has shape `(n,)`\n",
        "- $Y$ has shape `(n, output_dims)`. \n",
        "\n",
        "Let each row of $X, B, Y$ (or the 0$^\\text{th}$ dimension) represent a different sample. Disregarding bias, note how any given row in $Y$ is a set of linear combinations of the same row in $X$. In other words, a Dense Layer is high-dimensional Linear Regression. \n",
        "\n",
        "Once you've built up your model, try training and testing it in the code block below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "gYU8NKSdTc4W"
      },
      "outputs": [],
      "source": [
        "class LinearRegression(Regression, TrainTest):\n",
        "    def __init__(self, input_dims, output_dims):\n",
        "        super().__init__(input_dims, output_dims)\n",
        "        self.activation = nn.Identity()\n",
        "        self.loss = nn.MSELoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBUaTs3N_hmn",
        "outputId": "18463f27-c8fd-4486-85e4-c7dddede0f5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1/200] {'loss': tensor(28940.4727, grad_fn=<MseLossBackward0>), 'test_loss': 10209.982421875}\n",
            "[Epoch 2/200] {'loss': tensor(9482.0654, grad_fn=<MseLossBackward0>), 'test_loss': 7009.6953125}\n",
            "[Epoch 3/200] {'loss': tensor(6350.4385, grad_fn=<MseLossBackward0>), 'test_loss': 6460.5927734375}\n",
            "[Epoch 4/200] {'loss': tensor(5831.1694, grad_fn=<MseLossBackward0>), 'test_loss': 6345.5771484375}\n",
            "[Epoch 5/200] {'loss': tensor(5730.0400, grad_fn=<MseLossBackward0>), 'test_loss': 6304.16845703125}\n",
            "[Epoch 6/200] {'loss': tensor(5695.9839, grad_fn=<MseLossBackward0>), 'test_loss': 6276.33935546875}\n",
            "[Epoch 7/200] {'loss': tensor(5672.8345, grad_fn=<MseLossBackward0>), 'test_loss': 6251.5224609375}\n",
            "[Epoch 8/200] {'loss': tensor(5651.6001, grad_fn=<MseLossBackward0>), 'test_loss': 6227.63671875}\n",
            "[Epoch 9/200] {'loss': tensor(5630.8428, grad_fn=<MseLossBackward0>), 'test_loss': 6204.18701171875}\n",
            "[Epoch 10/200] {'loss': tensor(5610.3301, grad_fn=<MseLossBackward0>), 'test_loss': 6181.02734375}\n",
            "[Epoch 11/200] {'loss': tensor(5590.0220, grad_fn=<MseLossBackward0>), 'test_loss': 6158.10693359375}\n",
            "[Epoch 12/200] {'loss': tensor(5569.9116, grad_fn=<MseLossBackward0>), 'test_loss': 6135.40380859375}\n",
            "[Epoch 13/200] {'loss': tensor(5549.9961, grad_fn=<MseLossBackward0>), 'test_loss': 6112.9091796875}\n",
            "[Epoch 14/200] {'loss': tensor(5530.2725, grad_fn=<MseLossBackward0>), 'test_loss': 6090.6181640625}\n",
            "[Epoch 15/200] {'loss': tensor(5510.7388, grad_fn=<MseLossBackward0>), 'test_loss': 6068.5283203125}\n",
            "[Epoch 16/200] {'loss': tensor(5491.3926, grad_fn=<MseLossBackward0>), 'test_loss': 6046.63427734375}\n",
            "[Epoch 17/200] {'loss': tensor(5472.2339, grad_fn=<MseLossBackward0>), 'test_loss': 6024.9375}\n",
            "[Epoch 18/200] {'loss': tensor(5453.2578, grad_fn=<MseLossBackward0>), 'test_loss': 6003.43408203125}\n",
            "[Epoch 19/200] {'loss': tensor(5434.4644, grad_fn=<MseLossBackward0>), 'test_loss': 5982.12158203125}\n",
            "[Epoch 20/200] {'loss': tensor(5415.8521, grad_fn=<MseLossBackward0>), 'test_loss': 5960.99951171875}\n",
            "[Epoch 21/200] {'loss': tensor(5397.4170, grad_fn=<MseLossBackward0>), 'test_loss': 5940.064453125}\n",
            "[Epoch 22/200] {'loss': tensor(5379.1587, grad_fn=<MseLossBackward0>), 'test_loss': 5919.3154296875}\n",
            "[Epoch 23/200] {'loss': tensor(5361.0752, grad_fn=<MseLossBackward0>), 'test_loss': 5898.75}\n",
            "[Epoch 24/200] {'loss': tensor(5343.1641, grad_fn=<MseLossBackward0>), 'test_loss': 5878.36767578125}\n",
            "[Epoch 25/200] {'loss': tensor(5325.4238, grad_fn=<MseLossBackward0>), 'test_loss': 5858.1650390625}\n",
            "[Epoch 26/200] {'loss': tensor(5307.8525, grad_fn=<MseLossBackward0>), 'test_loss': 5838.14111328125}\n",
            "[Epoch 27/200] {'loss': tensor(5290.4487, grad_fn=<MseLossBackward0>), 'test_loss': 5818.2939453125}\n",
            "[Epoch 28/200] {'loss': tensor(5273.2100, grad_fn=<MseLossBackward0>), 'test_loss': 5798.62109375}\n",
            "[Epoch 29/200] {'loss': tensor(5256.1357, grad_fn=<MseLossBackward0>), 'test_loss': 5779.12255859375}\n",
            "[Epoch 30/200] {'loss': tensor(5239.2231, grad_fn=<MseLossBackward0>), 'test_loss': 5759.79443359375}\n",
            "[Epoch 31/200] {'loss': tensor(5222.4712, grad_fn=<MseLossBackward0>), 'test_loss': 5740.63671875}\n",
            "[Epoch 32/200] {'loss': tensor(5205.8774, grad_fn=<MseLossBackward0>), 'test_loss': 5721.64599609375}\n",
            "[Epoch 33/200] {'loss': tensor(5189.4414, grad_fn=<MseLossBackward0>), 'test_loss': 5702.8232421875}\n",
            "[Epoch 34/200] {'loss': tensor(5173.1602, grad_fn=<MseLossBackward0>), 'test_loss': 5684.16357421875}\n",
            "[Epoch 35/200] {'loss': tensor(5157.0327, grad_fn=<MseLossBackward0>), 'test_loss': 5665.66845703125}\n",
            "[Epoch 36/200] {'loss': tensor(5141.0571, grad_fn=<MseLossBackward0>), 'test_loss': 5647.33349609375}\n",
            "[Epoch 37/200] {'loss': tensor(5125.2314, grad_fn=<MseLossBackward0>), 'test_loss': 5629.1591796875}\n",
            "[Epoch 38/200] {'loss': tensor(5109.5552, grad_fn=<MseLossBackward0>), 'test_loss': 5611.14306640625}\n",
            "[Epoch 39/200] {'loss': tensor(5094.0269, grad_fn=<MseLossBackward0>), 'test_loss': 5593.28369140625}\n",
            "[Epoch 40/200] {'loss': tensor(5078.6436, grad_fn=<MseLossBackward0>), 'test_loss': 5575.57958984375}\n",
            "[Epoch 41/200] {'loss': tensor(5063.4048, grad_fn=<MseLossBackward0>), 'test_loss': 5558.02880859375}\n",
            "[Epoch 42/200] {'loss': tensor(5048.3086, grad_fn=<MseLossBackward0>), 'test_loss': 5540.6298828125}\n",
            "[Epoch 43/200] {'loss': tensor(5033.3535, grad_fn=<MseLossBackward0>), 'test_loss': 5523.3818359375}\n",
            "[Epoch 44/200] {'loss': tensor(5018.5381, grad_fn=<MseLossBackward0>), 'test_loss': 5506.2822265625}\n",
            "[Epoch 45/200] {'loss': tensor(5003.8608, grad_fn=<MseLossBackward0>), 'test_loss': 5489.33056640625}\n",
            "[Epoch 46/200] {'loss': tensor(4989.3208, grad_fn=<MseLossBackward0>), 'test_loss': 5472.525390625}\n",
            "[Epoch 47/200] {'loss': tensor(4974.9160, grad_fn=<MseLossBackward0>), 'test_loss': 5455.86474609375}\n",
            "[Epoch 48/200] {'loss': tensor(4960.6450, grad_fn=<MseLossBackward0>), 'test_loss': 5439.3466796875}\n",
            "[Epoch 49/200] {'loss': tensor(4946.5068, grad_fn=<MseLossBackward0>), 'test_loss': 5422.97119140625}\n",
            "[Epoch 50/200] {'loss': tensor(4932.4995, grad_fn=<MseLossBackward0>), 'test_loss': 5406.73583984375}\n",
            "[Epoch 51/200] {'loss': tensor(4918.6226, grad_fn=<MseLossBackward0>), 'test_loss': 5390.63916015625}\n",
            "[Epoch 52/200] {'loss': tensor(4904.8730, grad_fn=<MseLossBackward0>), 'test_loss': 5374.6806640625}\n",
            "[Epoch 53/200] {'loss': tensor(4891.2510, grad_fn=<MseLossBackward0>), 'test_loss': 5358.85791015625}\n",
            "[Epoch 54/200] {'loss': tensor(4877.7549, grad_fn=<MseLossBackward0>), 'test_loss': 5343.171875}\n",
            "[Epoch 55/200] {'loss': tensor(4864.3833, grad_fn=<MseLossBackward0>), 'test_loss': 5327.6181640625}\n",
            "[Epoch 56/200] {'loss': tensor(4851.1348, grad_fn=<MseLossBackward0>), 'test_loss': 5312.1962890625}\n",
            "[Epoch 57/200] {'loss': tensor(4838.0078, grad_fn=<MseLossBackward0>), 'test_loss': 5296.90576171875}\n",
            "[Epoch 58/200] {'loss': tensor(4825.0015, grad_fn=<MseLossBackward0>), 'test_loss': 5281.74560546875}\n",
            "[Epoch 59/200] {'loss': tensor(4812.1147, grad_fn=<MseLossBackward0>), 'test_loss': 5266.71337890625}\n",
            "[Epoch 60/200] {'loss': tensor(4799.3462, grad_fn=<MseLossBackward0>), 'test_loss': 5251.80810546875}\n",
            "[Epoch 61/200] {'loss': tensor(4786.6938, grad_fn=<MseLossBackward0>), 'test_loss': 5237.029296875}\n",
            "[Epoch 62/200] {'loss': tensor(4774.1572, grad_fn=<MseLossBackward0>), 'test_loss': 5222.375}\n",
            "[Epoch 63/200] {'loss': tensor(4761.7354, grad_fn=<MseLossBackward0>), 'test_loss': 5207.84375}\n",
            "[Epoch 64/200] {'loss': tensor(4749.4272, grad_fn=<MseLossBackward0>), 'test_loss': 5193.435546875}\n",
            "[Epoch 65/200] {'loss': tensor(4737.2300, grad_fn=<MseLossBackward0>), 'test_loss': 5179.1484375}\n",
            "[Epoch 66/200] {'loss': tensor(4725.1440, grad_fn=<MseLossBackward0>), 'test_loss': 5164.98046875}\n",
            "[Epoch 67/200] {'loss': tensor(4713.1680, grad_fn=<MseLossBackward0>), 'test_loss': 5150.931640625}\n",
            "[Epoch 68/200] {'loss': tensor(4701.3003, grad_fn=<MseLossBackward0>), 'test_loss': 5137.00048828125}\n",
            "[Epoch 69/200] {'loss': tensor(4689.5396, grad_fn=<MseLossBackward0>), 'test_loss': 5123.18603515625}\n",
            "[Epoch 70/200] {'loss': tensor(4677.8867, grad_fn=<MseLossBackward0>), 'test_loss': 5109.48583984375}\n",
            "[Epoch 71/200] {'loss': tensor(4666.3379, grad_fn=<MseLossBackward0>), 'test_loss': 5095.90087890625}\n",
            "[Epoch 72/200] {'loss': tensor(4654.8936, grad_fn=<MseLossBackward0>), 'test_loss': 5082.42822265625}\n",
            "[Epoch 73/200] {'loss': tensor(4643.5522, grad_fn=<MseLossBackward0>), 'test_loss': 5069.0673828125}\n",
            "[Epoch 74/200] {'loss': tensor(4632.3125, grad_fn=<MseLossBackward0>), 'test_loss': 5055.818359375}\n",
            "[Epoch 75/200] {'loss': tensor(4621.1743, grad_fn=<MseLossBackward0>), 'test_loss': 5042.67822265625}\n",
            "[Epoch 76/200] {'loss': tensor(4610.1357, grad_fn=<MseLossBackward0>), 'test_loss': 5029.6474609375}\n",
            "[Epoch 77/200] {'loss': tensor(4599.1958, grad_fn=<MseLossBackward0>), 'test_loss': 5016.724609375}\n",
            "[Epoch 78/200] {'loss': tensor(4588.3535, grad_fn=<MseLossBackward0>), 'test_loss': 5003.9072265625}\n",
            "[Epoch 79/200] {'loss': tensor(4577.6084, grad_fn=<MseLossBackward0>), 'test_loss': 4991.19580078125}\n",
            "[Epoch 80/200] {'loss': tensor(4566.9595, grad_fn=<MseLossBackward0>), 'test_loss': 4978.58984375}\n",
            "[Epoch 81/200] {'loss': tensor(4556.4053, grad_fn=<MseLossBackward0>), 'test_loss': 4966.0869140625}\n",
            "[Epoch 82/200] {'loss': tensor(4545.9438, grad_fn=<MseLossBackward0>), 'test_loss': 4953.68603515625}\n",
            "[Epoch 83/200] {'loss': tensor(4535.5767, grad_fn=<MseLossBackward0>), 'test_loss': 4941.3876953125}\n",
            "[Epoch 84/200] {'loss': tensor(4525.3008, grad_fn=<MseLossBackward0>), 'test_loss': 4929.1884765625}\n",
            "[Epoch 85/200] {'loss': tensor(4515.1157, grad_fn=<MseLossBackward0>), 'test_loss': 4917.08984375}\n",
            "[Epoch 86/200] {'loss': tensor(4505.0205, grad_fn=<MseLossBackward0>), 'test_loss': 4905.09033203125}\n",
            "[Epoch 87/200] {'loss': tensor(4495.0146, grad_fn=<MseLossBackward0>), 'test_loss': 4893.1875}\n",
            "[Epoch 88/200] {'loss': tensor(4485.0972, grad_fn=<MseLossBackward0>), 'test_loss': 4881.3818359375}\n",
            "[Epoch 89/200] {'loss': tensor(4475.2666, grad_fn=<MseLossBackward0>), 'test_loss': 4869.671875}\n",
            "[Epoch 90/200] {'loss': tensor(4465.5229, grad_fn=<MseLossBackward0>), 'test_loss': 4858.056640625}\n",
            "[Epoch 91/200] {'loss': tensor(4455.8643, grad_fn=<MseLossBackward0>), 'test_loss': 4846.53662109375}\n",
            "[Epoch 92/200] {'loss': tensor(4446.2905, grad_fn=<MseLossBackward0>), 'test_loss': 4835.10888671875}\n",
            "[Epoch 93/200] {'loss': tensor(4436.8003, grad_fn=<MseLossBackward0>), 'test_loss': 4823.77294921875}\n",
            "[Epoch 94/200] {'loss': tensor(4427.3931, grad_fn=<MseLossBackward0>), 'test_loss': 4812.5283203125}\n",
            "[Epoch 95/200] {'loss': tensor(4418.0679, grad_fn=<MseLossBackward0>), 'test_loss': 4801.37451171875}\n",
            "[Epoch 96/200] {'loss': tensor(4408.8237, grad_fn=<MseLossBackward0>), 'test_loss': 4790.310546875}\n",
            "[Epoch 97/200] {'loss': tensor(4399.6602, grad_fn=<MseLossBackward0>), 'test_loss': 4779.3349609375}\n",
            "[Epoch 98/200] {'loss': tensor(4390.5757, grad_fn=<MseLossBackward0>), 'test_loss': 4768.44677734375}\n",
            "[Epoch 99/200] {'loss': tensor(4381.5703, grad_fn=<MseLossBackward0>), 'test_loss': 4757.64599609375}\n",
            "[Epoch 100/200] {'loss': tensor(4372.6426, grad_fn=<MseLossBackward0>), 'test_loss': 4746.93115234375}\n",
            "[Epoch 101/200] {'loss': tensor(4363.7925, grad_fn=<MseLossBackward0>), 'test_loss': 4736.302734375}\n",
            "[Epoch 102/200] {'loss': tensor(4355.0186, grad_fn=<MseLossBackward0>), 'test_loss': 4725.7578125}\n",
            "[Epoch 103/200] {'loss': tensor(4346.3188, grad_fn=<MseLossBackward0>), 'test_loss': 4715.296875}\n",
            "[Epoch 104/200] {'loss': tensor(4337.6953, grad_fn=<MseLossBackward0>), 'test_loss': 4704.9189453125}\n",
            "[Epoch 105/200] {'loss': tensor(4329.1455, grad_fn=<MseLossBackward0>), 'test_loss': 4694.623046875}\n",
            "[Epoch 106/200] {'loss': tensor(4320.6694, grad_fn=<MseLossBackward0>), 'test_loss': 4684.40869140625}\n",
            "[Epoch 107/200] {'loss': tensor(4312.2651, grad_fn=<MseLossBackward0>), 'test_loss': 4674.275390625}\n",
            "[Epoch 108/200] {'loss': tensor(4303.9321, grad_fn=<MseLossBackward0>), 'test_loss': 4664.2216796875}\n",
            "[Epoch 109/200] {'loss': tensor(4295.6714, grad_fn=<MseLossBackward0>), 'test_loss': 4654.24658203125}\n",
            "[Epoch 110/200] {'loss': tensor(4287.4805, grad_fn=<MseLossBackward0>), 'test_loss': 4644.35009765625}\n",
            "[Epoch 111/200] {'loss': tensor(4279.3589, grad_fn=<MseLossBackward0>), 'test_loss': 4634.53173828125}\n",
            "[Epoch 112/200] {'loss': tensor(4271.3062, grad_fn=<MseLossBackward0>), 'test_loss': 4624.78955078125}\n",
            "[Epoch 113/200] {'loss': tensor(4263.3223, grad_fn=<MseLossBackward0>), 'test_loss': 4615.12353515625}\n",
            "[Epoch 114/200] {'loss': tensor(4255.4053, grad_fn=<MseLossBackward0>), 'test_loss': 4605.533203125}\n",
            "[Epoch 115/200] {'loss': tensor(4247.5562, grad_fn=<MseLossBackward0>), 'test_loss': 4596.01806640625}\n",
            "[Epoch 116/200] {'loss': tensor(4239.7720, grad_fn=<MseLossBackward0>), 'test_loss': 4586.57666015625}\n",
            "[Epoch 117/200] {'loss': tensor(4232.0532, grad_fn=<MseLossBackward0>), 'test_loss': 4577.2080078125}\n",
            "[Epoch 118/200] {'loss': tensor(4224.3994, grad_fn=<MseLossBackward0>), 'test_loss': 4567.912109375}\n",
            "[Epoch 119/200] {'loss': tensor(4216.8105, grad_fn=<MseLossBackward0>), 'test_loss': 4558.68896484375}\n",
            "[Epoch 120/200] {'loss': tensor(4209.2842, grad_fn=<MseLossBackward0>), 'test_loss': 4549.53564453125}\n",
            "[Epoch 121/200] {'loss': tensor(4201.8213, grad_fn=<MseLossBackward0>), 'test_loss': 4540.4541015625}\n",
            "[Epoch 122/200] {'loss': tensor(4194.4199, grad_fn=<MseLossBackward0>), 'test_loss': 4531.44287109375}\n",
            "[Epoch 123/200] {'loss': tensor(4187.0811, grad_fn=<MseLossBackward0>), 'test_loss': 4522.5}\n",
            "[Epoch 124/200] {'loss': tensor(4179.8027, grad_fn=<MseLossBackward0>), 'test_loss': 4513.62548828125}\n",
            "[Epoch 125/200] {'loss': tensor(4172.5850, grad_fn=<MseLossBackward0>), 'test_loss': 4504.8203125}\n",
            "[Epoch 126/200] {'loss': tensor(4165.4272, grad_fn=<MseLossBackward0>), 'test_loss': 4496.08203125}\n",
            "[Epoch 127/200] {'loss': tensor(4158.3276, grad_fn=<MseLossBackward0>), 'test_loss': 4487.40966796875}\n",
            "[Epoch 128/200] {'loss': tensor(4151.2871, grad_fn=<MseLossBackward0>), 'test_loss': 4478.8037109375}\n",
            "[Epoch 129/200] {'loss': tensor(4144.3047, grad_fn=<MseLossBackward0>), 'test_loss': 4470.26318359375}\n",
            "[Epoch 130/200] {'loss': tensor(4137.3794, grad_fn=<MseLossBackward0>), 'test_loss': 4461.7880859375}\n",
            "[Epoch 131/200] {'loss': tensor(4130.5107, grad_fn=<MseLossBackward0>), 'test_loss': 4453.376953125}\n",
            "[Epoch 132/200] {'loss': tensor(4123.6987, grad_fn=<MseLossBackward0>), 'test_loss': 4445.02978515625}\n",
            "[Epoch 133/200] {'loss': tensor(4116.9424, grad_fn=<MseLossBackward0>), 'test_loss': 4436.74560546875}\n",
            "[Epoch 134/200] {'loss': tensor(4110.2407, grad_fn=<MseLossBackward0>), 'test_loss': 4428.5234375}\n",
            "[Epoch 135/200] {'loss': tensor(4103.5942, grad_fn=<MseLossBackward0>), 'test_loss': 4420.36376953125}\n",
            "[Epoch 136/200] {'loss': tensor(4097.0015, grad_fn=<MseLossBackward0>), 'test_loss': 4412.265625}\n",
            "[Epoch 137/200] {'loss': tensor(4090.4624, grad_fn=<MseLossBackward0>), 'test_loss': 4404.22802734375}\n",
            "[Epoch 138/200] {'loss': tensor(4083.9758, grad_fn=<MseLossBackward0>), 'test_loss': 4396.25}\n",
            "[Epoch 139/200] {'loss': tensor(4077.5425, grad_fn=<MseLossBackward0>), 'test_loss': 4388.3330078125}\n",
            "[Epoch 140/200] {'loss': tensor(4071.1602, grad_fn=<MseLossBackward0>), 'test_loss': 4380.4736328125}\n",
            "[Epoch 141/200] {'loss': tensor(4064.8293, grad_fn=<MseLossBackward0>), 'test_loss': 4372.673828125}\n",
            "[Epoch 142/200] {'loss': tensor(4058.5496, grad_fn=<MseLossBackward0>), 'test_loss': 4364.931640625}\n",
            "[Epoch 143/200] {'loss': tensor(4052.3206, grad_fn=<MseLossBackward0>), 'test_loss': 4357.248046875}\n",
            "[Epoch 144/200] {'loss': tensor(4046.1406, grad_fn=<MseLossBackward0>), 'test_loss': 4349.6201171875}\n",
            "[Epoch 145/200] {'loss': tensor(4040.0107, grad_fn=<MseLossBackward0>), 'test_loss': 4342.04931640625}\n",
            "[Epoch 146/200] {'loss': tensor(4033.9292, grad_fn=<MseLossBackward0>), 'test_loss': 4334.5341796875}\n",
            "[Epoch 147/200] {'loss': tensor(4027.8965, grad_fn=<MseLossBackward0>), 'test_loss': 4327.07421875}\n",
            "[Epoch 148/200] {'loss': tensor(4021.9114, grad_fn=<MseLossBackward0>), 'test_loss': 4319.66943359375}\n",
            "[Epoch 149/200] {'loss': tensor(4015.9739, grad_fn=<MseLossBackward0>), 'test_loss': 4312.31884765625}\n",
            "[Epoch 150/200] {'loss': tensor(4010.0833, grad_fn=<MseLossBackward0>), 'test_loss': 4305.02294921875}\n",
            "[Epoch 151/200] {'loss': tensor(4004.2390, grad_fn=<MseLossBackward0>), 'test_loss': 4297.779296875}\n",
            "[Epoch 152/200] {'loss': tensor(3998.4412, grad_fn=<MseLossBackward0>), 'test_loss': 4290.58935546875}\n",
            "[Epoch 153/200] {'loss': tensor(3992.6885, grad_fn=<MseLossBackward0>), 'test_loss': 4283.4521484375}\n",
            "[Epoch 154/200] {'loss': tensor(3986.9817, grad_fn=<MseLossBackward0>), 'test_loss': 4276.3662109375}\n",
            "[Epoch 155/200] {'loss': tensor(3981.3179, grad_fn=<MseLossBackward0>), 'test_loss': 4269.3330078125}\n",
            "[Epoch 156/200] {'loss': tensor(3975.7000, grad_fn=<MseLossBackward0>), 'test_loss': 4262.34912109375}\n",
            "[Epoch 157/200] {'loss': tensor(3970.1252, grad_fn=<MseLossBackward0>), 'test_loss': 4255.4169921875}\n",
            "[Epoch 158/200] {'loss': tensor(3964.5942, grad_fn=<MseLossBackward0>), 'test_loss': 4248.53515625}\n",
            "[Epoch 159/200] {'loss': tensor(3959.1062, grad_fn=<MseLossBackward0>), 'test_loss': 4241.7021484375}\n",
            "[Epoch 160/200] {'loss': tensor(3953.6604, grad_fn=<MseLossBackward0>), 'test_loss': 4234.91845703125}\n",
            "[Epoch 161/200] {'loss': tensor(3948.2563, grad_fn=<MseLossBackward0>), 'test_loss': 4228.18408203125}\n",
            "[Epoch 162/200] {'loss': tensor(3942.8948, grad_fn=<MseLossBackward0>), 'test_loss': 4221.498046875}\n",
            "[Epoch 163/200] {'loss': tensor(3937.5745, grad_fn=<MseLossBackward0>), 'test_loss': 4214.859375}\n",
            "[Epoch 164/200] {'loss': tensor(3932.2942, grad_fn=<MseLossBackward0>), 'test_loss': 4208.26806640625}\n",
            "[Epoch 165/200] {'loss': tensor(3927.0552, grad_fn=<MseLossBackward0>), 'test_loss': 4201.724609375}\n",
            "[Epoch 166/200] {'loss': tensor(3921.8567, grad_fn=<MseLossBackward0>), 'test_loss': 4195.22705078125}\n",
            "[Epoch 167/200] {'loss': tensor(3916.6968, grad_fn=<MseLossBackward0>), 'test_loss': 4188.77587890625}\n",
            "[Epoch 168/200] {'loss': tensor(3911.5771, grad_fn=<MseLossBackward0>), 'test_loss': 4182.37060546875}\n",
            "[Epoch 169/200] {'loss': tensor(3906.4954, grad_fn=<MseLossBackward0>), 'test_loss': 4176.01123046875}\n",
            "[Epoch 170/200] {'loss': tensor(3901.4534, grad_fn=<MseLossBackward0>), 'test_loss': 4169.69580078125}\n",
            "[Epoch 171/200] {'loss': tensor(3896.4490, grad_fn=<MseLossBackward0>), 'test_loss': 4163.42578125}\n",
            "[Epoch 172/200] {'loss': tensor(3891.4822, grad_fn=<MseLossBackward0>), 'test_loss': 4157.19970703125}\n",
            "[Epoch 173/200] {'loss': tensor(3886.5532, grad_fn=<MseLossBackward0>), 'test_loss': 4151.017578125}\n",
            "[Epoch 174/200] {'loss': tensor(3881.6611, grad_fn=<MseLossBackward0>), 'test_loss': 4144.87841796875}\n",
            "[Epoch 175/200] {'loss': tensor(3876.8057, grad_fn=<MseLossBackward0>), 'test_loss': 4138.7822265625}\n",
            "[Epoch 176/200] {'loss': tensor(3871.9868, grad_fn=<MseLossBackward0>), 'test_loss': 4132.7294921875}\n",
            "[Epoch 177/200] {'loss': tensor(3867.2039, grad_fn=<MseLossBackward0>), 'test_loss': 4126.71826171875}\n",
            "[Epoch 178/200] {'loss': tensor(3862.4565, grad_fn=<MseLossBackward0>), 'test_loss': 4120.7490234375}\n",
            "[Epoch 179/200] {'loss': tensor(3857.7446, grad_fn=<MseLossBackward0>), 'test_loss': 4114.82177734375}\n",
            "[Epoch 180/200] {'loss': tensor(3853.0679, grad_fn=<MseLossBackward0>), 'test_loss': 4108.9345703125}\n",
            "[Epoch 181/200] {'loss': tensor(3848.4253, grad_fn=<MseLossBackward0>), 'test_loss': 4103.08935546875}\n",
            "[Epoch 182/200] {'loss': tensor(3843.8169, grad_fn=<MseLossBackward0>), 'test_loss': 4097.28369140625}\n",
            "[Epoch 183/200] {'loss': tensor(3839.2432, grad_fn=<MseLossBackward0>), 'test_loss': 4091.517822265625}\n",
            "[Epoch 184/200] {'loss': tensor(3834.7034, grad_fn=<MseLossBackward0>), 'test_loss': 4085.792236328125}\n",
            "[Epoch 185/200] {'loss': tensor(3830.1963, grad_fn=<MseLossBackward0>), 'test_loss': 4080.1064453125}\n",
            "[Epoch 186/200] {'loss': tensor(3825.7224, grad_fn=<MseLossBackward0>), 'test_loss': 4074.458984375}\n",
            "[Epoch 187/200] {'loss': tensor(3821.2805, grad_fn=<MseLossBackward0>), 'test_loss': 4068.850341796875}\n",
            "[Epoch 188/200] {'loss': tensor(3816.8718, grad_fn=<MseLossBackward0>), 'test_loss': 4063.2802734375}\n",
            "[Epoch 189/200] {'loss': tensor(3812.4954, grad_fn=<MseLossBackward0>), 'test_loss': 4057.748291015625}\n",
            "[Epoch 190/200] {'loss': tensor(3808.1501, grad_fn=<MseLossBackward0>), 'test_loss': 4052.25341796875}\n",
            "[Epoch 191/200] {'loss': tensor(3803.8364, grad_fn=<MseLossBackward0>), 'test_loss': 4046.79638671875}\n",
            "[Epoch 192/200] {'loss': tensor(3799.5537, grad_fn=<MseLossBackward0>), 'test_loss': 4041.3759765625}\n",
            "[Epoch 193/200] {'loss': tensor(3795.3025, grad_fn=<MseLossBackward0>), 'test_loss': 4035.9921875}\n",
            "[Epoch 194/200] {'loss': tensor(3791.0818, grad_fn=<MseLossBackward0>), 'test_loss': 4030.64501953125}\n",
            "[Epoch 195/200] {'loss': tensor(3786.8909, grad_fn=<MseLossBackward0>), 'test_loss': 4025.33349609375}\n",
            "[Epoch 196/200] {'loss': tensor(3782.7305, grad_fn=<MseLossBackward0>), 'test_loss': 4020.058349609375}\n",
            "[Epoch 197/200] {'loss': tensor(3778.5999, grad_fn=<MseLossBackward0>), 'test_loss': 4014.818359375}\n",
            "[Epoch 198/200] {'loss': tensor(3774.4983, grad_fn=<MseLossBackward0>), 'test_loss': 4009.61376953125}\n",
            "[Epoch 199/200] {'loss': tensor(3770.4260, grad_fn=<MseLossBackward0>), 'test_loss': 4004.44384765625}\n",
            "[Epoch 200/200] {'loss': tensor(3766.3821, grad_fn=<MseLossBackward0>), 'test_loss': 3999.308349609375}\n"
          ]
        }
      ],
      "source": [
        "torch_model = LinearRegression(X0.shape[-1], 1)\n",
        "torch_model.set_learning_rate(0.3)\n",
        "torch_model.train_test(\n",
        "    [[torch.Tensor(X0), torch.Tensor(Y0)]], \n",
        "    [[torch.Tensor(X1), torch.Tensor(Y1)]],\n",
        "    epochs=200\n",
        ");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H63TERfyvZtS"
      },
      "source": [
        "## Shapes That Might Be Useful..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtwbP5tRbJky"
      },
      "source": [
        "Throughout the duration of this course, you might find it really helpful to check the shapes of each of your different tensors and outputs just to verify that everything is working as intended. Check the block below to see an example!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBkTktWcD9dz",
        "outputId": "a2934e58-31aa-43ad-cbae-d6538ddbbcb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> Prediction Shape: torch.Size([353, 1])\n",
            "> Weights    Shape: torch.Size([1, 10])\n",
            "> Bias       Shape: torch.Size([1])\n",
            "> Loss       Shape: torch.Size([])\n"
          ]
        }
      ],
      "source": [
        "y_true = torch.Tensor(Y0)\n",
        "y_pred = torch_model(torch.Tensor(X0))\n",
        "loss = torch_model.loss(y_true, y_pred)\n",
        "\n",
        "print(f\"\"\"\n",
        "> Prediction Shape: {y_pred.shape}\n",
        "> Weights    Shape: {list(torch_model.parameters())[0].shape}\n",
        "> Bias       Shape: {list(torch_model.parameters())[1].shape}\n",
        "> Loss       Shape: {loss.shape}\n",
        "\"\"\".strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cu1pLK46fFqX"
      },
      "source": [
        "Next, let's start building up all the different parts of the basic PyTorch tools that we used in order to see what's under the hood."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZfozqWdLB2V"
      },
      "source": [
        "## PyTorch Complexity Assumptions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVv6iQios7-D"
      },
      "source": [
        "### Tensors\n",
        "- Tensors are responsible for maintaining their own gradients\n",
        "- Tensors hold on to `backward` functions to which they can pass a gradient into. These backwards functions are provided by the layers associated with those tensors. \n",
        "    - If a tensor is a terminal node, it will pass in an upstream gradient of `None`.\n",
        "    - If a tensor is a non-terminal node, it will pass the accumulated upstream gradient. \n",
        "    - `backward` functions as a linked list algorithm and crawls back the chain, computing the gradient for every tensor that it hits (as long as they require a gradient).\n",
        "- Since the tensors hold their own gradients, the optimizer can merely take the tensors' values, take their gradients, and then just optimize them.\n",
        "- However, because tensors are always keeping track of their gradients when `requires_grad` is set to `True`, we also want to add a way to stop tracking gradients.\n",
        "    - For instance, while evaluating the performance of our model, we want to make sure that the model doesn't learn anything from this evaluation phase.\n",
        "    - We can use the `no_grad` subclass to automatically handle the flipping of this `requires_grad` value. \n",
        "        - Every time we enter a `with Tensor.no_grad():` block, the code within `no_grad`'s `__enter__()` method will execute. \n",
        "        - Once we exit the same `with Tensor.no_grad():` block, `no_grad`'s `__exit()__` method will run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "5ZbMLm30y3tD"
      },
      "outputs": [],
      "source": [
        "class Tensor(np.ndarray):\n",
        "\n",
        "    '''\n",
        "    Subclassing numpy arrays is a bit weird:\n",
        "    https://numpy.org/doc/stable/user/basics.subclassing.html\n",
        "\n",
        "    Just assume that the attributes referred to in __new__/__array_finalize__ \n",
        "    will be accessible in a Tensor when a new Tensor object is created.  \n",
        "    '''\n",
        "\n",
        "    requires_grad = True  ## Class variable; accessible by Tensor.requires_grad\n",
        "\n",
        "    def __new__(cls, input_array):\n",
        "        obj = np.asarray(input_array).view(cls)\n",
        "        obj.backward = lambda x: None   ## Backward starts as None, gets assigned later\n",
        "        obj.grad = None                 ## Gradient starts as None, gets computed later\n",
        "        obj.requires_grad = True        ## By default, we'll want to compute gradient for new tensors\n",
        "        obj.to = lambda x: obj          ## We don't handle special device support (i.e. cpu vs gpu/cuda)\n",
        "        return obj\n",
        "\n",
        "    def __array_finalize__(self, obj):\n",
        "        if obj is None: return\n",
        "        self.backward       = getattr(obj, 'backward',      lambda x: None)\n",
        "        self.to             = getattr(obj, 'to',            lambda x: obj)\n",
        "        self.grad           = getattr(obj, 'grad',          None)\n",
        "        self.requires_grad  = getattr(obj, 'requires_grad', None)\n",
        "\n",
        "    class no_grad():\n",
        "\n",
        "        '''\n",
        "        Synergizes with Tensor: By entering the tensor with no_grad scope, \n",
        "        the Tensor.requires_grad singleton will swap to False. \n",
        "        '''\n",
        "        \n",
        "        def __enter__(self):\n",
        "            # When tape scope is entered, stop asking tensors to record gradients\n",
        "            Tensor.requires_grad = False\n",
        "            return self\n",
        "\n",
        "        def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "            # When tape scope is exited, let Diffable start recording to self.operation\n",
        "            Tensor.requires_grad = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovL7L6STvXCU"
      },
      "source": [
        "### Diffable\n",
        "\n",
        "Let's specify a \"Diffable\" object which will represent a module that can be differentiated. This class will make the following assumptions: \n",
        "- Gradients will need to flow through the input pathways in order to compute earlier gradients. \n",
        "    - Therefore, inputs will need an appropriate \"backward\"\n",
        "- Parameters will need to recieve gradients.\n",
        "- More specifically, if a `Diffable` object performs an operation on some input, then we know that the gradient from the output of the Diffable w.r.t. the inputs is the gradient of the `Diffable`'s operations w.r.t. its inputs.\n",
        "  - Thus, a `Diffable`'s `input_gradients()` function should return a tuple with each of the partial derivatives of the operations performed in the forward pass."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "9v68mc_U1ymJ"
      },
      "outputs": [],
      "source": [
        "from abc import ABC, abstractmethod  # # For abstract method support\n",
        "\n",
        "class Diffable(ABC):\n",
        "    \"\"\"\n",
        "        We use these to represent differentiable layers which we can compute gradients for.\n",
        "    \"\"\"\n",
        "\n",
        "    def to(self, device):\n",
        "        return self         # Just there to ignore device setting calls\n",
        "    \n",
        "    def __call__(self, *args, **kwargs):\n",
        "        \n",
        "        ## The call method keeps track of method inputs and outputs\n",
        "        self.argnames   = self.forward.__code__.co_varnames[1:]\n",
        "        named_args      = {self.argnames[i] : args[i] for i in range(len(args))}\n",
        "        self.input_dict = {**named_args, **kwargs}\n",
        "        self.inputs     = [self.input_dict[arg] for arg in self.argnames if arg in self.input_dict.keys()]\n",
        "        self.outputs    = self.forward(*args, **kwargs)\n",
        "\n",
        "        ## Make sure outputs are tensors and tie back to this layer\n",
        "        list_outs = isinstance(self.outputs, list) or isinstance(self.outputs, tuple)\n",
        "        if not list_outs:\n",
        "            self.outputs = [self.outputs]\n",
        "        self.outputs = [Tensor(out) for out in self.outputs]\n",
        "        for out in self.outputs: \n",
        "            out.backward = self.backward\n",
        "\n",
        "        # print(self.__class__.__name__.ljust(24), [v.shape for v in self.inputs], '->', [v.shape for v in self.outputs])\n",
        "            \n",
        "        ## And then finally, it returns the output, thereby wrapping the forward\n",
        "        return self.outputs if list_outs else self.outputs[0]\n",
        "\n",
        "    def parameters(self):\n",
        "        \"\"\"Returns a list of parameters\"\"\"\n",
        "        return ()\n",
        "\n",
        "    @abstractmethod\n",
        "    def forward(self, x):\n",
        "        \"\"\"Pass inputs through function. Can store inputs and outputs as instance variables\"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def input_gradients(self):\n",
        "        \"\"\"Returns local gradient of layer output w.r.t. input\"\"\"\n",
        "        pass\n",
        "\n",
        "    def weight_gradients(self):\n",
        "        \"\"\"Returns local gradient of layer output w.r.t. weights\"\"\"\n",
        "        return []\n",
        "    \n",
        "    @abstractmethod\n",
        "    def backward(self, grad=np.array([[1]])):\n",
        "        \"\"\"\n",
        "        Propagate upstream gradient backwards by composing with local gradient\n",
        "        \n",
        "        SCAFFOLD: \n",
        "\n",
        "        Differentiate with respect to layer parameters:\n",
        "            For every param-gradient pair\n",
        "            - If all Tensors or this tensor do not require gradients, then skip\n",
        "            - Otherwise, compose upstream and local gradient\n",
        "        \n",
        "        Differentiate with respect to layer input:\n",
        "            For every input-gradient pair\n",
        "            - If all Tensors or this tensor do not require gradients, then skip\n",
        "            - Otherwise, compose upstream and local gradient\n",
        "\n",
        "        Usefulseful print boilerplate...: \n",
        "            # print(f'Diffing w.r.t. \"{k}\": local = {g.shape} and upstream = {grad.shape}')\n",
        "        \"\"\"\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2n50HNq3ZuCO"
      },
      "source": [
        "### Loss\n",
        "**[TODO]:** Implement the forward pass in `forward()`.\n",
        "- The forward pass should just give the mean squared error between `y_pred` and `y_true`.\n",
        "\n",
        "**[TODO]:** Implement the backward pass in `backward()`.\n",
        "- This should take advantage of the layer inputs as well as the gradients computed with respect to them.\n",
        "- Feel free to only work with the input gradients, since this loss layer does not have any parameters.\n",
        "\n",
        "**[TODO]:** Calculate and return `input_gradients()`:\n",
        "- You want to calculate the gradients which flow to the inputs: `y_pred` and `y_true`\n",
        "- Note that we don't want to \"train\" `y_true`, so you can just return 0 for the grads for `y_true`\n",
        "- Return the partial derivative of mean squared error w.r.t. `y_pred`, and 0.\n",
        "\n",
        "Note that we don't need to implement `weight_gradients()` here because MSELoss doesn't have weights!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "1GG1E4VaC9hs"
      },
      "outputs": [],
      "source": [
        "class MSELoss(Diffable):\n",
        "\n",
        "    \"\"\"\n",
        "        Calculates mean squared error loss and gradient w.r.t. inputs.\n",
        "        Subclasses Diffable.\n",
        "    \"\"\"\n",
        "\n",
        "    def forward(self, y_pred, y_true):\n",
        "        \"\"\"Mean squared error forward pass!\"\"\"\n",
        "        # TODO: Compute and return the MSE given predicted and actual labels\n",
        "\n",
        "        return np.mean((y_pred-y_true)**2)\n",
        "\n",
        "    def input_gradients(self):\n",
        "        \"\"\"Mean squared error backpropagation!\"\"\"\n",
        "        # TODO: Compute and return the gradients\n",
        "\n",
        "        number_of_samples = len(self.inputs[1])\n",
        "        pd_y_pred = (-2/number_of_samples) * (self.inputs[1]-self.inputs[0])\n",
        "        pd_y_true = np.array([[0]])\n",
        "        return pd_y_pred, pd_y_true\n",
        "\n",
        "    def backward(self, grad=np.array([[1]])):\n",
        "        \"\"\"Mean squared error backpropagation!\"\"\"        \n",
        "        ## TODO: Differentiate with respect to layer inputs        \n",
        "        ## For each input value and input gradient\n",
        "            ## Compose the upstream gradient with this input's gradient\n",
        "            ## Set the gradient of the tensor to the composed gradient as necessary\n",
        "            ## Pass the composed gradient backward through structure\n",
        "\n",
        "        pdes = self.input_gradients()\n",
        "        for i, input in enumerate(self.inputs):\n",
        "          upstream_gradient = pdes[i] @ grad\n",
        "          if input.requires_grad: \n",
        "            input.grad = upstream_gradient\n",
        "          input.backward(upstream_gradient)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CHqCz0nKsGB"
      },
      "source": [
        "And here are some sanity checks you can run to make sure that your code is working as intended. In the first check, the outputs should match. In the second, they should be within the specified range."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "45Y7ngPUKehB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2914e097-f21a-4fae-f840-c4181dbca9e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(3762.3677, grad_fn=<MseLossBackward0>)\n",
            "3762.367317212915\n"
          ]
        }
      ],
      "source": [
        "class con: \n",
        "    ## Control set using default PyTorch\n",
        "    ytrue = torch.Tensor(Y0)\n",
        "    ypred = torch_model(torch.Tensor(X0))\n",
        "    loss_fn = nn.MSELoss()\n",
        "\n",
        "class exp: \n",
        "    ## Experimental set using your own implementation\n",
        "    ytrue = Tensor(Y0)\n",
        "    ypred = Tensor(con.ypred.detach().numpy())\n",
        "    loss_fn = MSELoss()\n",
        "\n",
        "def ypred_to_loss(ns):\n",
        "    ## Compute loss using the control and experimental namespaces\n",
        "    ns.loss = ns.loss_fn(ns.ypred, ns.ytrue)\n",
        "    return ns.loss\n",
        "\n",
        "## Sanity Check 1: Make sure that the forward pass is the same (i.e. your implementation matches the control)\n",
        "print(ypred_to_loss(con))\n",
        "print(ypred_to_loss(exp))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "P9tUAGJkg6I9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bfd3631-9fa8-4d8d-efb8-d6c4980f5eff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum difference 3.174411677964173e-08 should be less than 0.00001\n"
          ]
        }
      ],
      "source": [
        "## Sanity Check 2: Make sure that the backwards pass is the same\n",
        "\n",
        "con.ypred = con.ypred.detach()\n",
        "con.ypred.requires_grad = True\n",
        "# print(\"Before running backwards:\\n\", con.ypred.grad)\n",
        "ypred_to_loss(con)\n",
        "con.loss.backward()\n",
        "# print(\"After running backwards:\\n\", con.ypred.grad)\n",
        "\n",
        "exp.ypred.grad = None\n",
        "# print(\"Before running backwards:\\n\", np.round(exp.ypred.grad, 4))\n",
        "ypred_to_loss(exp)\n",
        "exp.loss.backward()\n",
        "# print(\"After running backwards:\\n\", np.round(exp.ypred.grad, 4))\n",
        "\n",
        "max_diff = np.max(exp.ypred.grad - con.ypred.grad.detach().numpy())\n",
        "print(f\"Maximum difference {max_diff} should be less than 0.00001\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inej4Q37gqN8"
      },
      "source": [
        "### Linear Layer\n",
        "Next, a linear layer!\n",
        "\n",
        "**[TODO]:** Implement the `forward()` pass of a linear layer. \n",
        "\n",
        "**[TODO]:** Calculate (manually) the weight gradients.\n",
        "- Manually differentiate the Dense layer with respect to weights and biases.\n",
        "- Return weight gradient, then bias gradient in that order\n",
        "- HINT: How is differentiating with matrix variables similar to and different from normal differentiation?\n",
        "\n",
        "**[TODO]:** Initalize weights and biases in `_initialize_weight()`.\n",
        "- In a linear layer, we have 2 parameters: weights and biases. \n",
        "- Return two NumPy arrays of the correct shapes according according to the function's arguments. \n",
        "- Return weights and biases, in that order.\n",
        "\n",
        "**[TODO]:** Implement the backward function.\n",
        "- Feel free to only work with the weight gradients, since we do not yet need to support multilayered networks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "50FnJezypdpt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ad60bb5-d1da-4e5c-b979-11a3517b519e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum difference 2.1583385745316264e-08 should be less than 0.00001\n",
            "\n",
            "Losses: Control 28989.435546875 vs Experimental 28989.434802946595\n",
            "\n",
            "Control Params:\n",
            "Parameter containing:\n",
            "tensor([[-0.3109,  0.1411, -0.0018,  0.0307,  0.1978,  0.2675, -0.2526, -0.1572,\n",
            "         -0.0281, -0.0158]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.3118], requires_grad=True)\n",
            "\n",
            "Experimental Params:\n",
            "[[-0.3108711   0.14111896 -0.00178832  0.03069019  0.19783439  0.26753297\n",
            "  -0.2526343  -0.1571826  -0.02807947 -0.01575226]]\n",
            "[-0.311823]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Linear(Diffable):\n",
        "\n",
        "    \"\"\"\n",
        "        Standard linear/dense layer.\n",
        "        Subclasses Diffable.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, out_features, device=None, dtype=None):\n",
        "        self.w, self.b = self.__class__._initialize_weight(in_features, out_features)\n",
        "    \n",
        "    def parameters(self):\n",
        "        return self.w, self.b\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"Forward pass for a dense layer! Refer to lecture slides for how this is computed.\"\"\"\n",
        "        # TODO: implement the forward pass and return the outputs\n",
        "        weights = self.w\n",
        "        bias = self.b\n",
        "        # print(weights.T.shape, bias.shape)\n",
        "        return inputs @ weights.T + bias\n",
        "\n",
        "    def weight_gradients(self):\n",
        "        \"\"\"Calculating the gradients of the weights and biases!\"\"\"\n",
        "        # TODO: Implement calculation of gradients\n",
        "        # partial derivative of y hat with respect to weights\n",
        "        wgrads = self.inputs[0] * np.ones_like(self.w)\n",
        "        # with respect to bias\n",
        "        bgrads = [1] * np.ones_like(self.b)\n",
        "\n",
        "        return (wgrads, bgrads)\n",
        "\n",
        "    def input_gradients(self):\n",
        "        \"\"\"Calculate the gradients of the inputs! (Not necessary for HW1)\"\"\"\n",
        "        return (self.w,)\n",
        "\n",
        "    @staticmethod\n",
        "    def _initialize_weight(input_size, output_size):\n",
        "        \"\"\"\n",
        "        Initializes the values of the weights and biases. You can assume that \n",
        "        bias is a zero-vector and weight is normally-distributed.\n",
        "        \"\"\"\n",
        "        ## TODO: Implement default assumption: zero-init for bias, normal distribution for weights\n",
        "        ## Must return tensors for tracking purposes.\n",
        "\n",
        "        default_weights = Tensor(np.random.normal(size=(output_size, input_size)))\n",
        "        default_bias = Tensor(np.zeros((output_size, 1)))\n",
        "        return default_weights, default_bias\n",
        "\n",
        "    def backward(self, grad=np.array([[1]])):\n",
        "        ## For every weight/bias and weight/bias gradient\n",
        "            ## Compose the upstream gradient with this weight's/bias's gradient\n",
        "            ## Set the gradient of the tensor to the composed gradient if necessary\n",
        "            ## Backpropagate the composed gradient through the structure \n",
        "\n",
        "        wg = self.weight_gradients() \n",
        "\n",
        "        for i, input in enumerate(self.parameters()):\n",
        "          upstream_gradient = np.sum(wg[i] * grad, 0, keepdims=True)\n",
        "          if input.requires_grad:  \n",
        "            input.grad = upstream_gradient\n",
        "          input.backward(upstream_gradient) \n",
        "\n",
        "class con:\n",
        "    ## Control set using regular pytorch\n",
        "    X0 = torch.Tensor(X0)\n",
        "    Y0 = torch.Tensor(Y0)\n",
        "    X0.requires_grad = True\n",
        "    Y0.requires_grad = True\n",
        "    dense = nn.Linear(10, 1)\n",
        "    loss_fn = nn.MSELoss()\n",
        "\n",
        "class exp:\n",
        "    ## Experimental set using your own implementation\n",
        "    X0 = Tensor(X0)\n",
        "    Y0 = Tensor(Y0)\n",
        "    dense = Linear(10, 1)\n",
        "    dense.w, dense.b = [Tensor(p.detach().numpy()) for p in con.dense.parameters()]\n",
        "    loss_fn = MSELoss()\n",
        "\n",
        "def x_to_loss(ns):\n",
        "    ns.ypred = ns.dense(ns.X0)\n",
        "    ns.loss  = ns.loss_fn(ns.ypred, ns.Y0)\n",
        "    return ns.loss\n",
        "\n",
        "x_to_loss(con)\n",
        "x_to_loss(exp)\n",
        "\n",
        "## Sanity Check 1: Make sure that the forward pass is the same\n",
        "# print(con.ypred)\n",
        "# print(exp.ypred)\n",
        "\n",
        "print(f\"Maximum difference {np.max(con.ypred.detach().numpy() - exp.ypred)} should be less than 0.00001\\n\")\n",
        "\n",
        "print(f\"Losses: Control {con.loss} vs Experimental {exp.loss}\")\n",
        "\n",
        "print('\\nControl Params:',      *list(con.dense.parameters()), sep='\\n')\n",
        "print('\\nExperimental Params:', *list(exp.dense.parameters()), sep='\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "-v6oUfSJxidG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eaa56bfc-a09f-4a48-b27a-07d207f69247"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After running backwards on weights:\n",
            "[tensor([[-1.1565, -0.2473, -4.0642, -2.9841, -2.0397, -1.8101,  3.1655, -3.8406,\n",
            "         -4.2051, -2.5258]]), tensor([-304.5309])]\n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "After running backwards on weights:\n",
            "[Tensor([[-1.15645516, -0.24732176, -4.06418752, -2.98412365, -2.03967723,\n",
            "         -1.81007177,  3.16551135, -3.84063986, -4.20513317, -2.5258354 ]]), Tensor([[-304.53089639]])]\n"
          ]
        }
      ],
      "source": [
        "## Sanity Check 2: Make sure that the backwards pass is the same\n",
        "\n",
        "con.X0 = con.X0.detach()\n",
        "con.Y0 = con.Y0.detach()\n",
        "for p in con.dense.parameters():\n",
        "    if p.grad is None: continue\n",
        "    p.grad.detach_()\n",
        "    p.grad = None\n",
        "\n",
        "x_to_loss(con).backward()\n",
        "print(\"After running backwards on weights:\")  \n",
        "print([p.grad for p in con.dense.parameters()])\n",
        "\n",
        "for p in exp.dense.parameters(): p.grad = None\n",
        "x_to_loss(exp).backward()\n",
        "\n",
        "print(\"\\n\" + \"*\" * 100 + \"\\n\")\n",
        "print(\"After running backwards on weights:\")  \n",
        "print([p.grad for p in exp.dense.parameters()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQGhTAMgFe_i"
      },
      "source": [
        "## Optimizing With The Gradients\n",
        "\n",
        "To use the gradients we calculated previously, we need an optimizer. The optimizer allows us to update our weights and bias. A simple approach could be to simply subtract the gradient from the weights and bias. In doing so, we follow the gradient in its opposite direction, minimizing loss. This is what is called gradient descent. \n",
        "\n",
        "However, simply subtracting the gradients from the weights could result in the weights changing wildly between each sample, making training longer. To prevent this, we use a learning rate. The learning rate is a hyperparameter that specifies how much a single step updates weights. A smaller learning rate means that the gradients have less of an impact on the weights, and vice versa.\n",
        "\n",
        "Of course, this is just one (simple) approach. In a later lab, you'll learn about other optimizers, such as Adam and RMSProp.\n",
        "\n",
        "**[TODO]:** Implement stochastic gradient descent for each parameter using the learning rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "J5s6o4_WGXhe"
      },
      "outputs": [],
      "source": [
        "class SGD: \n",
        "    \"\"\"\n",
        "        Performs stochastic gradident descent with the specified learning rate.\n",
        "    \"\"\"\n",
        "    def __init__(self, params, lr, *args, **kwargs):\n",
        "        self.params = params\n",
        "        self.lr = lr\n",
        "    \n",
        "    def zero_grad(self):\n",
        "        \"\"\"\n",
        "            Reset the gradients.\n",
        "        \"\"\"\n",
        "        for param in self.params:\n",
        "          param.grad = 0\n",
        "            \n",
        "    def step(self):\n",
        "        \"\"\"\n",
        "            Update paramaters by subtracting the gradient multiplied by the learning rate.\n",
        "        \"\"\"\n",
        "        ## TODO: Implement stochastic grad descent for each parameter\n",
        "\n",
        "        for param in self.params:\n",
        "          param -= param.grad * self.lr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9m63xegnLnj"
      },
      "source": [
        "Below, you'll use your new implementations to optimize for linear regression manually. FakeTorchModule will also be provided to make some of the mimicking process easier. \n",
        "\n",
        "**[TODO]:** Complete the model and compare this model's performance to the previous `LinearRegression` model -- they should have a similar loss after training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "M6-a0DfvFlnN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82600ae5-afa9-44c3-9097-29a488cfdeb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 1/500] {'loss': Tensor(28898.86692306), 'test_loss': 14887.682156362702}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 2/500] {'loss': Tensor(14104.54425197), 'test_loss': 9486.761363377895}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 3/500] {'loss': Tensor(8769.28551963), 'test_loss': 7515.878842582132}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 4/500] {'loss': Tensor(6839.29029337), 'test_loss': 6786.12722666752}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 5/500] {'loss': Tensor(6135.22877421), 'test_loss': 6507.056016032061}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 6/500] {'loss': Tensor(5872.55576822), 'test_loss': 6392.612000681713}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 7/500] {'loss': Tensor(5768.83960507), 'test_loss': 6338.907372863466}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 8/500] {'loss': Tensor(5722.40626871), 'test_loss': 6307.989170027001}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 9/500] {'loss': Tensor(5696.65334343), 'test_loss': 6285.857478695963}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 10/500] {'loss': Tensor(5678.40377582), 'test_loss': 6267.267305612528}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 11/500] {'loss': Tensor(5662.91349581), 'test_loss': 6250.20564530546}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 12/500] {'loss': Tensor(5648.47425631), 'test_loss': 6233.871800259434}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 13/500] {'loss': Tensor(5634.47069009), 'test_loss': 6217.931172163148}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 14/500] {'loss': Tensor(5620.68086261), 'test_loss': 6202.234993385452}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 15/500] {'loss': Tensor(5607.02447556), 'test_loss': 6186.712356070169}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 16/500] {'loss': Tensor(5593.47222169), 'test_loss': 6171.327042066236}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 17/500] {'loss': Tensor(5580.01315312), 'test_loss': 6156.059368026365}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 18/500] {'loss': Tensor(5566.64293415), 'test_loss': 6140.898061553948}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 19/500] {'loss': Tensor(5553.35961223), 'test_loss': 6125.836371899633}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 20/500] {'loss': Tensor(5540.16209566), 'test_loss': 6110.8700839001895}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 21/500] {'loss': Tensor(5527.0496054), 'test_loss': 6095.996447219113}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 22/500] {'loss': Tensor(5514.02147782), 'test_loss': 6081.213574878523}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 23/500] {'loss': Tensor(5501.07709362), 'test_loss': 6066.520095621277}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 24/500] {'loss': Tensor(5488.21585221), 'test_loss': 6051.914949189671}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 25/500] {'loss': Tensor(5475.43716251), 'test_loss': 6037.397264338749}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 26/500] {'loss': Tensor(5462.74043955), 'test_loss': 6022.966285621052}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 27/500] {'loss': Tensor(5450.12510332), 'test_loss': 6008.621329250363}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 28/500] {'loss': Tensor(5437.59057825), 'test_loss': 5994.361756423766}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 29/500] {'loss': Tensor(5425.13629305), 'test_loss': 5980.186957168649}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 30/500] {'loss': Tensor(5412.76168061), 'test_loss': 5966.096340550213}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 31/500] {'loss': Tensor(5400.46617797), 'test_loss': 5952.089328728079}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 32/500] {'loss': Tensor(5388.24922626), 'test_loss': 5938.1653533440585}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 33/500] {'loss': Tensor(5376.11027068), 'test_loss': 5924.323853322296}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 34/500] {'loss': Tensor(5364.04876046), 'test_loss': 5910.564273525308}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 35/500] {'loss': Tensor(5352.06414887), 'test_loss': 5896.886063928763}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 36/500] {'loss': Tensor(5340.1558931), 'test_loss': 5883.288679110639}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 37/500] {'loss': Tensor(5328.32345435), 'test_loss': 5869.7715779309765}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 38/500] {'loss': Tensor(5316.56629768), 'test_loss': 5856.334223327151}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 39/500] {'loss': Tensor(5304.88389209), 'test_loss': 5842.976082179214}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 40/500] {'loss': Tensor(5293.27571041), 'test_loss': 5829.696625217739}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 41/500] {'loss': Tensor(5281.74122932), 'test_loss': 5816.495326957515}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 42/500] {'loss': Tensor(5270.2799293), 'test_loss': 5803.371665646927}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 43/500] {'loss': Tensor(5258.8912946), 'test_loss': 5790.325123226944}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 44/500] {'loss': Tensor(5247.57481323), 'test_loss': 5777.355185295984}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 45/500] {'loss': Tensor(5236.32997692), 'test_loss': 5764.461341078409}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 46/500] {'loss': Tensor(5225.15628109), 'test_loss': 5751.643083395306}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 47/500] {'loss': Tensor(5214.05322484), 'test_loss': 5738.899908636704}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 48/500] {'loss': Tensor(5203.02031089), 'test_loss': 5726.231316734751}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 49/500] {'loss': Tensor(5192.05704559), 'test_loss': 5713.636811137543}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 50/500] {'loss': Tensor(5181.16293888), 'test_loss': 5701.115898783402}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 51/500] {'loss': Tensor(5170.33750426), 'test_loss': 5688.668090075528}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 52/500] {'loss': Tensor(5159.58025874), 'test_loss': 5676.292898856912}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 53/500] {'loss': Tensor(5148.89072289), 'test_loss': 5663.989842385515}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 54/500] {'loss': Tensor(5138.2684207), 'test_loss': 5651.7584413096465}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 55/500] {'loss': Tensor(5127.71287968), 'test_loss': 5639.598219643556}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 56/500] {'loss': Tensor(5117.22363073), 'test_loss': 5627.508704743209}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 57/500] {'loss': Tensor(5106.80020818), 'test_loss': 5615.489427282246}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 58/500] {'loss': Tensor(5096.44214972), 'test_loss': 5603.539921228127}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 59/500] {'loss': Tensor(5086.14899643), 'test_loss': 5591.65972381845}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 60/500] {'loss': Tensor(5075.9202927), 'test_loss': 5579.84837553744}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 61/500] {'loss': Tensor(5065.75558623), 'test_loss': 5568.1054200926155}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 62/500] {'loss': Tensor(5055.65442801), 'test_loss': 5556.430404391619}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 63/500] {'loss': Tensor(5045.61637229), 'test_loss': 5544.822878519226}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 64/500] {'loss': Tensor(5035.64097656), 'test_loss': 5533.2823957145065}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 65/500] {'loss': Tensor(5025.72780151), 'test_loss': 5521.808512348169}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 66/500] {'loss': Tensor(5015.87641103), 'test_loss': 5510.400787900058}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 67/500] {'loss': Tensor(5006.08637218), 'test_loss': 5499.058784936818}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 68/500] {'loss': Tensor(4996.35725515), 'test_loss': 5487.782069089721}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 69/500] {'loss': Tensor(4986.68863325), 'test_loss': 5476.570209032656}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 70/500] {'loss': Tensor(4977.0800829), 'test_loss': 5465.422776460277}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 71/500] {'loss': Tensor(4967.53118358), 'test_loss': 5454.3393460663065}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 72/500] {'loss': Tensor(4958.04151783), 'test_loss': 5443.319495522002}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 73/500] {'loss': Tensor(4948.61067122), 'test_loss': 5432.362805454781}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 74/500] {'loss': Tensor(4939.23823232), 'test_loss': 5421.468859426988}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 75/500] {'loss': Tensor(4929.92379268), 'test_loss': 5410.637243914832}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 76/500] {'loss': Tensor(4920.66694684), 'test_loss': 5399.867548287468}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 77/500] {'loss': Tensor(4911.46729225), 'test_loss': 5389.159364786231}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 78/500] {'loss': Tensor(4902.32442929), 'test_loss': 5378.512288504026}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 79/500] {'loss': Tensor(4893.23796124), 'test_loss': 5367.925917364859}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 80/500] {'loss': Tensor(4884.20749427), 'test_loss': 5357.399852103525}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 81/500] {'loss': Tensor(4875.23263738), 'test_loss': 5346.933696245435}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 82/500] {'loss': Tensor(4866.31300242), 'test_loss': 5336.527056086606}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 83/500] {'loss': Tensor(4857.44820406), 'test_loss': 5326.179540673768}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 84/500] {'loss': Tensor(4848.63785976), 'test_loss': 5315.89076178465}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 85/500] {'loss': Tensor(4839.88158973), 'test_loss': 5305.660333908376}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 86/500] {'loss': Tensor(4831.17901696), 'test_loss': 5295.48787422603}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 87/500] {'loss': Tensor(4822.52976718), 'test_loss': 5285.373002591347}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 88/500] {'loss': Tensor(4813.9334688), 'test_loss': 5275.315341511544}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 89/500] {'loss': Tensor(4805.38975294), 'test_loss': 5265.314516128302}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 90/500] {'loss': Tensor(4796.89825341), 'test_loss': 5255.370154198871}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 91/500] {'loss': Tensor(4788.45860663), 'test_loss': 5245.4818860773275}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 92/500] {'loss': Tensor(4780.07045171), 'test_loss': 5235.649344695949}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 93/500] {'loss': Tensor(4771.73343033), 'test_loss': 5225.872165546749}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 94/500] {'loss': Tensor(4763.44718678), 'test_loss': 5216.149986663118}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 95/500] {'loss': Tensor(4755.21136793), 'test_loss': 5206.4824486016205}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 96/500] {'loss': Tensor(4747.02562321), 'test_loss': 5196.86919442391}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 97/500] {'loss': Tensor(4738.88960457), 'test_loss': 5187.309869678785}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 98/500] {'loss': Tensor(4730.80296651), 'test_loss': 5177.804122384367}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 99/500] {'loss': Tensor(4722.76536601), 'test_loss': 5168.351603010411}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 100/500] {'loss': Tensor(4714.77646254), 'test_loss': 5158.951964460753}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 101/500] {'loss': Tensor(4706.83591803), 'test_loss': 5149.604862055859}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 102/500] {'loss': Tensor(4698.94339687), 'test_loss': 5140.309953515541}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 103/500] {'loss': Tensor(4691.09856587), 'test_loss': 5131.066898941755}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 104/500] {'loss': Tensor(4683.30109425), 'test_loss': 5121.875360801561}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 105/500] {'loss': Tensor(4675.55065362), 'test_loss': 5112.7350039101775}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 106/500] {'loss': Tensor(4667.84691799), 'test_loss': 5103.645495414178}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 107/500] {'loss': Tensor(4660.18956371), 'test_loss': 5094.6065047748025}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 108/500] {'loss': Tensor(4652.57826946), 'test_loss': 5085.617703751389}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 109/500] {'loss': Tensor(4645.01271627), 'test_loss': 5076.678766384925}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 110/500] {'loss': Tensor(4637.49258747), 'test_loss': 5067.789368981728}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 111/500] {'loss': Tensor(4630.01756866), 'test_loss': 5058.949190097226}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 112/500] {'loss': Tensor(4622.58734775), 'test_loss': 5050.15791051987}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 113/500] {'loss': Tensor(4615.20161487), 'test_loss': 5041.415213255162}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 114/500] {'loss': Tensor(4607.86006242), 'test_loss': 5032.720783509792}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 115/500] {'loss': Tensor(4600.562385), 'test_loss': 5024.074308675895}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 116/500] {'loss': Tensor(4593.30827945), 'test_loss': 5015.475478315424}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 117/500] {'loss': Tensor(4586.09744476), 'test_loss': 5006.923984144627}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 118/500] {'loss': Tensor(4578.92958214), 'test_loss': 4998.41952001865}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 119/500] {'loss': Tensor(4571.80439492), 'test_loss': 4989.961781916238}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 120/500] {'loss': Tensor(4564.7215886), 'test_loss': 4981.550467924561}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 121/500] {'loss': Tensor(4557.6808708), 'test_loss': 4973.1852782241385}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 122/500] {'loss': Tensor(4550.68195125), 'test_loss': 4964.865915073869}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 123/500] {'loss': Tensor(4543.72454178), 'test_loss': 4956.592082796194}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 124/500] {'loss': Tensor(4536.80835631), 'test_loss': 4948.363487762336}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 125/500] {'loss': Tensor(4529.9331108), 'test_loss': 4940.179838377668}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 126/500] {'loss': Tensor(4523.0985233), 'test_loss': 4932.040845067177}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 127/500] {'loss': Tensor(4516.30431386), 'test_loss': 4923.946220261035}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 128/500] {'loss': Tensor(4509.55020458), 'test_loss': 4915.8956783802805}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 129/500] {'loss': Tensor(4502.83591955), 'test_loss': 4907.888935822593}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 130/500] {'loss': Tensor(4496.16118485), 'test_loss': 4899.9257109481805}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 131/500] {'loss': Tensor(4489.52572856), 'test_loss': 4892.005724065763}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 132/500] {'loss': Tensor(4482.9292807), 'test_loss': 4884.128697418665}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 133/500] {'loss': Tensor(4476.37157324), 'test_loss': 4876.294355170993}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 134/500] {'loss': Tensor(4469.85234009), 'test_loss': 4868.502423393935}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 135/500] {'loss': Tensor(4463.37131709), 'test_loss': 4860.752630052141}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 136/500] {'loss': Tensor(4456.92824198), 'test_loss': 4853.044704990213}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 137/500] {'loss': Tensor(4450.52285437), 'test_loss': 4845.378379919286}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 138/500] {'loss': Tensor(4444.15489579), 'test_loss': 4837.753388403707}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 139/500] {'loss': Tensor(4437.82410959), 'test_loss': 4830.169465847822}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 140/500] {'loss': Tensor(4431.53024102), 'test_loss': 4822.626349482838}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 141/500] {'loss': Tensor(4425.27303711), 'test_loss': 4815.1237783538}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 142/500] {'loss': Tensor(4419.05224677), 'test_loss': 4807.661493306649}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 143/500] {'loss': Tensor(4412.86762068), 'test_loss': 4800.239236975379}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 144/500] {'loss': Tensor(4406.71891135), 'test_loss': 4792.856753769287}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 145/500] {'loss': Tensor(4400.60587303), 'test_loss': 4785.513789860317}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 146/500] {'loss': Tensor(4394.52826179), 'test_loss': 4778.210093170487}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 147/500] {'loss': Tensor(4388.48583543), 'test_loss': 4770.945413359424}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 148/500] {'loss': Tensor(4382.47835351), 'test_loss': 4763.7195018119655}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 149/500] {'loss': Tensor(4376.50557729), 'test_loss': 4756.532111625874}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 150/500] {'loss': Tensor(4370.5672698), 'test_loss': 4749.382997599627}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 151/500] {'loss': Tensor(4364.66319575), 'test_loss': 4742.271916220296}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 152/500] {'loss': Tensor(4358.79312153), 'test_loss': 4735.19862565152}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 153/500] {'loss': Tensor(4352.95681524), 'test_loss': 4728.16288572156}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 154/500] {'loss': Tensor(4347.15404664), 'test_loss': 4721.16445791144}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 155/500] {'loss': Tensor(4341.38458714), 'test_loss': 4714.203105343183}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 156/500] {'loss': Tensor(4335.64820981), 'test_loss': 4707.27859276811}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 157/500] {'loss': Tensor(4329.94468935), 'test_loss': 4700.390686555257}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 158/500] {'loss': Tensor(4324.27380207), 'test_loss': 4693.539154679844}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 159/500] {'loss': Tensor(4318.6353259), 'test_loss': 4686.7237667118425}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 160/500] {'loss': Tensor(4313.02904037), 'test_loss': 4679.944293804626}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 161/500] {'loss': Tensor(4307.4547266), 'test_loss': 4673.200508683704}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 162/500] {'loss': Tensor(4301.91216726), 'test_loss': 4666.49218563553}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 163/500] {'loss': Tensor(4296.40114662), 'test_loss': 4659.819100496389}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 164/500] {'loss': Tensor(4290.92145048), 'test_loss': 4653.181030641381}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 165/500] {'loss': Tensor(4285.47286618), 'test_loss': 4646.577754973467}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 166/500] {'loss': Tensor(4280.05518259), 'test_loss': 4640.009053912603}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 167/500] {'loss': Tensor(4274.66819011), 'test_loss': 4633.474709384954}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 168/500] {'loss': Tensor(4269.31168064), 'test_loss': 4626.974504812173}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 169/500] {'loss': Tensor(4263.98544756), 'test_loss': 4620.50822510078}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 170/500] {'loss': Tensor(4258.68928577), 'test_loss': 4614.075656631595}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 171/500] {'loss': Tensor(4253.42299161), 'test_loss': 4607.676587249262}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 172/500] {'loss': Tensor(4248.18636291), 'test_loss': 4601.310806251842}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 173/500] {'loss': Tensor(4242.97919893), 'test_loss': 4594.978104380485}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 174/500] {'loss': Tensor(4237.80130038), 'test_loss': 4588.678273809174}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 175/500] {'loss': Tensor(4232.65246942), 'test_loss': 4582.411108134544}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 176/500] {'loss': Tensor(4227.5325096), 'test_loss': 4576.176402365773}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 177/500] {'loss': Tensor(4222.4412259), 'test_loss': 4569.9739529145545}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 178/500] {'loss': Tensor(4217.37842471), 'test_loss': 4563.803557585125}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 179/500] {'loss': Tensor(4212.34391378), 'test_loss': 4557.665015564383}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 180/500] {'loss': Tensor(4207.33750227), 'test_loss': 4551.558127412064}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 181/500] {'loss': Tensor(4202.35900068), 'test_loss': 4545.482695051004}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 182/500] {'loss': Tensor(4197.40822091), 'test_loss': 4539.438521757445}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 183/500] {'loss': Tensor(4192.48497617), 'test_loss': 4533.425412151445}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 184/500] {'loss': Tensor(4187.58908103), 'test_loss': 4527.443172187332}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 185/500] {'loss': Tensor(4182.72035139), 'test_loss': 4521.491609144229}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 186/500] {'loss': Tensor(4177.87860447), 'test_loss': 4515.570531616675}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 187/500] {'loss': Tensor(4173.0636588), 'test_loss': 4509.679749505262}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 188/500] {'loss': Tensor(4168.27533421), 'test_loss': 4503.8190740074015}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 189/500] {'loss': Tensor(4163.51345182), 'test_loss': 4497.988317608097}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 190/500] {'loss': Tensor(4158.77783403), 'test_loss': 4492.1872940708345}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 191/500] {'loss': Tensor(4154.06830453), 'test_loss': 4486.415818428505}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 192/500] {'loss': Tensor(4149.38468825), 'test_loss': 4480.673706974409}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 193/500] {'loss': Tensor(4144.7268114), 'test_loss': 4474.9607772533145}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 194/500] {'loss': Tensor(4140.09450141), 'test_loss': 4469.2768480525965}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 195/500] {'loss': Tensor(4135.48758697), 'test_loss': 4463.62173939342}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 196/500] {'loss': Tensor(4130.90589798), 'test_loss': 4457.995272522002}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 197/500] {'loss': Tensor(4126.34926556), 'test_loss': 4452.397269900931}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 198/500] {'loss': Tensor(4121.81752204), 'test_loss': 4446.827555200546}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 199/500] {'loss': Tensor(4117.31050097), 'test_loss': 4441.285953290387}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 200/500] {'loss': Tensor(4112.82803706), 'test_loss': 4435.772290230691}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 201/500] {'loss': Tensor(4108.36996623), 'test_loss': 4430.286393263975}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 202/500] {'loss': Tensor(4103.93612555), 'test_loss': 4424.828090806649}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 203/500] {'loss': Tensor(4099.52635329), 'test_loss': 4419.397212440714}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 204/500] {'loss': Tensor(4095.14048883), 'test_loss': 4413.993588905509}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 205/500] {'loss': Tensor(4090.77837275), 'test_loss': 4408.617052089516}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 206/500] {'loss': Tensor(4086.43984674), 'test_loss': 4403.267435022233}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 207/500] {'loss': Tensor(4082.12475362), 'test_loss': 4397.944571866094}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 208/500] {'loss': Tensor(4077.83293735), 'test_loss': 4392.648297908457}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 209/500] {'loss': Tensor(4073.564243), 'test_loss': 4387.378449553648}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 210/500] {'loss': Tensor(4069.31851674), 'test_loss': 4382.134864315057}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 211/500] {'loss': Tensor(4065.09560585), 'test_loss': 4376.917380807289}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 212/500] {'loss': Tensor(4060.89535869), 'test_loss': 4371.725838738391}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 213/500] {'loss': Tensor(4056.71762471), 'test_loss': 4366.560078902111}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 214/500] {'loss': Tensor(4052.56225445), 'test_loss': 4361.419943170223}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 215/500] {'loss': Tensor(4048.42909949), 'test_loss': 4356.305274484909}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 216/500] {'loss': Tensor(4044.31801249), 'test_loss': 4351.2159168512}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 217/500] {'loss': Tensor(4040.22884716), 'test_loss': 4346.151715329454}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 218/500] {'loss': Tensor(4036.16145824), 'test_loss': 4341.112516027908}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 219/500] {'loss': Tensor(4032.11570152), 'test_loss': 4336.098166095273}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 220/500] {'loss': Tensor(4028.09143382), 'test_loss': 4331.108513713386}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 221/500] {'loss': Tensor(4024.08851299), 'test_loss': 4326.143408089911}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 222/500] {'loss': Tensor(4020.10679786), 'test_loss': 4321.202699451099}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 223/500] {'loss': Tensor(4016.14614831), 'test_loss': 4316.286239034595}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 224/500] {'loss': Tensor(4012.20642519), 'test_loss': 4311.393879082298}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 225/500] {'loss': Tensor(4008.28749036), 'test_loss': 4306.525472833268}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 226/500] {'loss': Tensor(4004.38920665), 'test_loss': 4301.6808745167045}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 227/500] {'loss': Tensor(4000.51143788), 'test_loss': 4296.8599393449385}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 228/500] {'loss': Tensor(3996.65404883), 'test_loss': 4292.062523506514}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 229/500] {'loss': Tensor(3992.81690526), 'test_loss': 4287.28848415929}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 230/500] {'loss': Tensor(3988.99987387), 'test_loss': 4282.5376794236145}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 231/500] {'loss': Tensor(3985.20282232), 'test_loss': 4277.809968375526}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 232/500] {'loss': Tensor(3981.42561921), 'test_loss': 4273.105211040025}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 233/500] {'loss': Tensor(3977.66813407), 'test_loss': 4268.423268384373}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 234/500] {'loss': Tensor(3973.93023737), 'test_loss': 4263.7640023114645}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 235/500] {'loss': Tensor(3970.2118005), 'test_loss': 4259.127275653222}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 236/500] {'loss': Tensor(3966.51269575), 'test_loss': 4254.512952164052}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 237/500] {'loss': Tensor(3962.83279634), 'test_loss': 4249.920896514349}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 238/500] {'loss': Tensor(3959.17197639), 'test_loss': 4245.350974284035}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 239/500] {'loss': Tensor(3955.53011089), 'test_loss': 4240.803051956167}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 240/500] {'loss': Tensor(3951.90707576), 'test_loss': 4236.276996910559}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 241/500] {'loss': Tensor(3948.30274776), 'test_loss': 4231.772677417485}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 242/500] {'loss': Tensor(3944.71700457), 'test_loss': 4227.289962631397}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 243/500] {'loss': Tensor(3941.14972471), 'test_loss': 4222.828722584708}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 244/500] {'loss': Tensor(3937.60078756), 'test_loss': 4218.38882818161}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 245/500] {'loss': Tensor(3934.07007338), 'test_loss': 4213.970151191942}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 246/500] {'loss': Tensor(3930.55746327), 'test_loss': 4209.572564245095}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 247/500] {'loss': Tensor(3927.06283917), 'test_loss': 4205.195940823971}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 248/500] {'loss': Tensor(3923.58608386), 'test_loss': 4200.840155258975}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 249/500] {'loss': Tensor(3920.12708096), 'test_loss': 4196.5050827220575}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 250/500] {'loss': Tensor(3916.68571491), 'test_loss': 4192.190599220797}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 251/500] {'loss': Tensor(3913.26187098), 'test_loss': 4187.896581592523}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 252/500] {'loss': Tensor(3909.85543524), 'test_loss': 4183.622907498488}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 253/500] {'loss': Tensor(3906.46629457), 'test_loss': 4179.369455418071}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 254/500] {'loss': Tensor(3903.09433667), 'test_loss': 4175.136104643035}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 255/500] {'loss': Tensor(3899.73945002), 'test_loss': 4170.922735271815}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 256/500] {'loss': Tensor(3896.4015239), 'test_loss': 4166.729228203853}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 257/500] {'loss': Tensor(3893.08044836), 'test_loss': 4162.555465133973}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 258/500] {'loss': Tensor(3889.77611425), 'test_loss': 4158.401328546792}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 259/500] {'loss': Tensor(3886.48841318), 'test_loss': 4154.266701711178}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 260/500] {'loss': Tensor(3883.21723754), 'test_loss': 4150.151468674745}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 261/500] {'loss': Tensor(3879.96248048), 'test_loss': 4146.055514258386}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 262/500] {'loss': Tensor(3876.72403588), 'test_loss': 4141.978724050844}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 263/500] {'loss': Tensor(3873.50179842), 'test_loss': 4137.920984403329}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 264/500] {'loss': Tensor(3870.29566348), 'test_loss': 4133.882182424164}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 265/500] {'loss': Tensor(3867.10552722), 'test_loss': 4129.86220597348}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 266/500] {'loss': Tensor(3863.9312865), 'test_loss': 4125.860943657938}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 267/500] {'loss': Tensor(3860.77283894), 'test_loss': 4121.878284825497}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 268/500] {'loss': Tensor(3857.63008286), 'test_loss': 4117.91411956022}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 269/500] {'loss': Tensor(3854.50291731), 'test_loss': 4113.9683386771085}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 270/500] {'loss': Tensor(3851.39124207), 'test_loss': 4110.040833716982}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 271/500] {'loss': Tensor(3848.29495759), 'test_loss': 4106.131496941395}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 272/500] {'loss': Tensor(3845.21396507), 'test_loss': 4102.240221327585}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 273/500] {'loss': Tensor(3842.14816636), 'test_loss': 4098.36690056346}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 274/500] {'loss': Tensor(3839.09746406), 'test_loss': 4094.5114290426195}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 275/500] {'loss': Tensor(3836.0617614), 'test_loss': 4090.6737018594163}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 276/500] {'loss': Tensor(3833.04096234), 'test_loss': 4086.8536148040484}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 277/500] {'loss': Tensor(3830.03497149), 'test_loss': 4083.051064357687}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 278/500] {'loss': Tensor(3827.04369415), 'test_loss': 4079.2659476876474}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 279/500] {'loss': Tensor(3824.06703628), 'test_loss': 4075.498162642578}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 280/500] {'loss': Tensor(3821.1049045), 'test_loss': 4071.7476077476977}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 281/500] {'loss': Tensor(3818.15720609), 'test_loss': 4068.0141822000687}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 282/500] {'loss': Tensor(3815.22384901), 'test_loss': 4064.297785863888}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 283/500] {'loss': Tensor(3812.30474183), 'test_loss': 4060.598319265832}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 284/500] {'loss': Tensor(3809.3997938), 'test_loss': 4056.9156835904187}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 285/500] {'loss': Tensor(3806.50891477), 'test_loss': 4053.2497806754113}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 286/500] {'loss': Tensor(3803.63201527), 'test_loss': 4049.6005130072535}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 287/500] {'loss': Tensor(3800.76900643), 'test_loss': 4045.9677837165373}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 288/500] {'loss': Tensor(3797.91980001), 'test_loss': 4042.3514965734985}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 289/500] {'loss': Tensor(3795.08430841), 'test_loss': 4038.7515559835574}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 290/500] {'loss': Tensor(3792.26244463), 'test_loss': 4035.1678669828743}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 291/500] {'loss': Tensor(3789.45412228), 'test_loss': 4031.6003352339494}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 292/500] {'loss': Tensor(3786.65925559), 'test_loss': 4028.0488670212526}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 293/500] {'loss': Tensor(3783.87775939), 'test_loss': 4024.5133692468758}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 294/500] {'loss': Tensor(3781.10954911), 'test_loss': 4020.9937494262326}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 295/500] {'loss': Tensor(3778.35454077), 'test_loss': 4017.4899156837655}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 296/500] {'loss': Tensor(3775.61265099), 'test_loss': 4014.0017767487125}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 297/500] {'loss': Tensor(3772.88379695), 'test_loss': 4010.5292419508755}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 298/500] {'loss': Tensor(3770.16789646), 'test_loss': 4007.0722212164437}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 299/500] {'loss': Tensor(3767.46486786), 'test_loss': 4003.6306250638268}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 300/500] {'loss': Tensor(3764.7746301), 'test_loss': 4000.204364599532}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 301/500] {'loss': Tensor(3762.09710267), 'test_loss': 3996.793351514063}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 302/500] {'loss': Tensor(3759.43220564), 'test_loss': 3993.3974980778535}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 303/500] {'loss': Tensor(3756.77985965), 'test_loss': 3990.0167171372213}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 304/500] {'loss': Tensor(3754.13998589), 'test_loss': 3986.650922110365}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 305/500] {'loss': Tensor(3751.51250608), 'test_loss': 3983.300026983376}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 306/500] {'loss': Tensor(3748.89734253), 'test_loss': 3979.963946306284}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 307/500] {'loss': Tensor(3746.29441806), 'test_loss': 3976.6425951891356}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 308/500] {'loss': Tensor(3743.70365606), 'test_loss': 3973.3358892980923}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 309/500] {'loss': Tensor(3741.12498044), 'test_loss': 3970.043744851562}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 310/500] {'loss': Tensor(3738.55831564), 'test_loss': 3966.76607861636}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 311/500] {'loss': Tensor(3736.00358665), 'test_loss': 3963.5028079038916}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 312/500] {'loss': Tensor(3733.46071895), 'test_loss': 3960.253850566362}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 313/500] {'loss': Tensor(3730.92963859), 'test_loss': 3957.019124993019}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 314/500] {'loss': Tensor(3728.41027209), 'test_loss': 3953.798550106424}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 315/500] {'loss': Tensor(3725.90254652), 'test_loss': 3950.592045358737}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 316/500] {'loss': Tensor(3723.40638944), 'test_loss': 3947.3995307280406}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 317/500] {'loss': Tensor(3720.92172893), 'test_loss': 3944.2209267146877}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 318/500] {'loss': Tensor(3718.44849356), 'test_loss': 3941.0561543376703}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 319/500] {'loss': Tensor(3715.98661241), 'test_loss': 3937.905135131022}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 320/500] {'loss': Tensor(3713.53601506), 'test_loss': 3934.7677911402325}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 321/500] {'loss': Tensor(3711.09663156), 'test_loss': 3931.64404491871}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 322/500] {'loss': Tensor(3708.66839247), 'test_loss': 3928.5338195242484}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 323/500] {'loss': Tensor(3706.25122883), 'test_loss': 3925.437038515527}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 324/500] {'loss': Tensor(3703.84507217), 'test_loss': 3922.353625948637}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 325/500] {'loss': Tensor(3701.44985447), 'test_loss': 3919.2835063736334}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 326/500] {'loss': Tensor(3699.06550821), 'test_loss': 3916.2266048311053}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 327/500] {'loss': Tensor(3696.69196634), 'test_loss': 3913.1828468487784}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 328/500] {'loss': Tensor(3694.32916227), 'test_loss': 3910.1521584381344}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 329/500] {'loss': Tensor(3691.97702988), 'test_loss': 3907.1344660910645}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 330/500] {'loss': Tensor(3689.63550351), 'test_loss': 3904.129696776532}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 331/500] {'loss': Tensor(3687.30451794), 'test_loss': 3901.137777937275}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 332/500] {'loss': Tensor(3684.98400844), 'test_loss': 3898.1586374865233}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 333/500] {'loss': Tensor(3682.6739107), 'test_loss': 3895.1922038047337}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 334/500] {'loss': Tensor(3680.37416087), 'test_loss': 3892.238405736366}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 335/500] {'loss': Tensor(3678.08469555), 'test_loss': 3889.297172586665}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 336/500] {'loss': Tensor(3675.80545177), 'test_loss': 3886.3684341184685}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 337/500] {'loss': Tensor(3673.53636701), 'test_loss': 3883.452120549052}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 338/500] {'loss': Tensor(3671.27737917), 'test_loss': 3880.5481625469743}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 339/500] {'loss': Tensor(3669.0284266), 'test_loss': 3877.656491228961}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 340/500] {'loss': Tensor(3666.78944807), 'test_loss': 3874.7770381568075}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 341/500] {'loss': Tensor(3664.56038277), 'test_loss': 3871.909735334298}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 342/500] {'loss': Tensor(3662.34117031), 'test_loss': 3869.054515204157}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 343/500] {'loss': Tensor(3660.13175075), 'test_loss': 3866.2113106450097}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 344/500] {'loss': Tensor(3657.93206452), 'test_loss': 3863.3800549683724}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 345/500] {'loss': Tensor(3655.7420525), 'test_loss': 3860.560681915668}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 346/500] {'loss': Tensor(3653.56165597), 'test_loss': 3857.7531256552497}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 347/500] {'loss': Tensor(3651.39081661), 'test_loss': 3854.957320779458}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 348/500] {'loss': Tensor(3649.22947651), 'test_loss': 3852.173202301694}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 349/500] {'loss': Tensor(3647.07757816), 'test_loss': 3849.4007056535115}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 350/500] {'loss': Tensor(3644.93506446), 'test_loss': 3846.639766681736}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 351/500] {'loss': Tensor(3642.80187868), 'test_loss': 3843.8903216455965}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 352/500] {'loss': Tensor(3640.67796452), 'test_loss': 3841.1523072138825}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 353/500] {'loss': Tensor(3638.56326603), 'test_loss': 3838.425660462125}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 354/500] {'loss': Tensor(3636.45772768), 'test_loss': 3835.7103188697843}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 355/500] {'loss': Tensor(3634.3612943), 'test_loss': 3833.006220317474}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 356/500] {'loss': Tensor(3632.27391112), 'test_loss': 3830.3133030841923}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 357/500] {'loss': Tensor(3630.19552375), 'test_loss': 3827.6315058445834}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 358/500] {'loss': Tensor(3628.12607815), 'test_loss': 3824.9607676662035}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 359/500] {'loss': Tensor(3626.06552069), 'test_loss': 3822.301028006827}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 360/500] {'loss': Tensor(3624.01379808), 'test_loss': 3819.652226711752}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 361/500] {'loss': Tensor(3621.97085741), 'test_loss': 3817.014304011138}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 362/500] {'loss': Tensor(3619.93664614), 'test_loss': 3814.3872005173544}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 363/500] {'loss': Tensor(3617.9111121), 'test_loss': 3811.7708572223555}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 364/500] {'loss': Tensor(3615.89420345), 'test_loss': 3809.165215495069}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 365/500] {'loss': Tensor(3613.88586874), 'test_loss': 3806.570217078798}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 366/500] {'loss': Tensor(3611.88605685), 'test_loss': 3803.9858040886597}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 367/500] {'loss': Tensor(3609.89471704), 'test_loss': 3801.411919009019}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 368/500] {'loss': Tensor(3607.91179889), 'test_loss': 3798.848504690958}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 369/500] {'loss': Tensor(3605.93725236), 'test_loss': 3796.2955043497573}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 370/500] {'loss': Tensor(3603.97102771), 'test_loss': 3793.7528615623883}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 371/500] {'loss': Tensor(3602.0130756), 'test_loss': 3791.2205202650393}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 372/500] {'loss': Tensor(3600.06334697), 'test_loss': 3788.6984247506434}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 373/500] {'loss': Tensor(3598.12179314), 'test_loss': 3786.186519666433}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 374/500] {'loss': Tensor(3596.18836575), 'test_loss': 3783.6847500115073}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 375/500] {'loss': Tensor(3594.26301678), 'test_loss': 3781.1930611344214}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 376/500] {'loss': Tensor(3592.34569853), 'test_loss': 3778.7113987307907}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 377/500] {'loss': Tensor(3590.43636362), 'test_loss': 3776.2397088409066}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 378/500] {'loss': Tensor(3588.53496503), 'test_loss': 3773.777937847381}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 379/500] {'loss': Tensor(3586.64145602), 'test_loss': 3771.3260324727935}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 380/500] {'loss': Tensor(3584.7557902), 'test_loss': 3768.8839397773677}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 381/500] {'loss': Tensor(3582.8779215), 'test_loss': 3766.451607156658}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 382/500] {'loss': Tensor(3581.00780414), 'test_loss': 3764.0289823392536}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 383/500] {'loss': Tensor(3579.14539268), 'test_loss': 3761.616013384497}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 384/500] {'loss': Tensor(3577.29064197), 'test_loss': 3759.2126486802204}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 385/500] {'loss': Tensor(3575.44350719), 'test_loss': 3756.8188369405016}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 386/500] {'loss': Tensor(3573.60394382), 'test_loss': 3754.43452720343}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 387/500] {'loss': Tensor(3571.77190764), 'test_loss': 3752.0596688288915}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 388/500] {'loss': Tensor(3569.94735474), 'test_loss': 3749.694211496366}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 389/500] {'loss': Tensor(3568.1302415), 'test_loss': 3747.3381052027494}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 390/500] {'loss': Tensor(3566.32052461), 'test_loss': 3744.991300260176}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 391/500] {'loss': Tensor(3564.51816104), 'test_loss': 3742.6537472938758}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 392/500] {'loss': Tensor(3562.72310808), 'test_loss': 3740.325397240023}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 393/500] {'loss': Tensor(3560.93532329), 'test_loss': 3738.0062013436245}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 394/500] {'loss': Tensor(3559.15476452), 'test_loss': 3735.696111156408}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 395/500] {'loss': Tensor(3557.38138993), 'test_loss': 3733.3950785347256}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 396/500] {'loss': Tensor(3555.61515794), 'test_loss': 3731.1030556374794}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 397/500] {'loss': Tensor(3553.85602727), 'test_loss': 3728.819994924057}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 398/500] {'loss': Tensor(3552.10395691), 'test_loss': 3726.5458491522822}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 399/500] {'loss': Tensor(3550.35890613), 'test_loss': 3724.2805713763796}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 400/500] {'loss': Tensor(3548.6208345), 'test_loss': 3722.024114944956}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 401/500] {'loss': Tensor(3546.88970184), 'test_loss': 3719.7764334989984}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 402/500] {'loss': Tensor(3545.16546825), 'test_loss': 3717.53748096987}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 403/500] {'loss': Tensor(3543.44809411), 'test_loss': 3715.307211577349}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 404/500] {'loss': Tensor(3541.73754006), 'test_loss': 3713.0855798276516}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 405/500] {'loss': Tensor(3540.033767), 'test_loss': 3710.872540511493}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 406/500] {'loss': Tensor(3538.33673613), 'test_loss': 3708.6680487021426}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 407/500] {'loss': Tensor(3536.64640887), 'test_loss': 3706.472059753509}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 408/500] {'loss': Tensor(3534.96274693), 'test_loss': 3704.2845292982265}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 409/500] {'loss': Tensor(3533.28571228), 'test_loss': 3702.105413245763}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 410/500] {'loss': Tensor(3531.61526714), 'test_loss': 3699.934667780539}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 411/500] {'loss': Tensor(3529.95137397), 'test_loss': 3697.772249360056}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 412/500] {'loss': Tensor(3528.29399553), 'test_loss': 3695.6181147130446}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 413/500] {'loss': Tensor(3526.64309478), 'test_loss': 3693.47222083762}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 414/500] {'loss': Tensor(3524.99863497), 'test_loss': 3691.334524999456}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 415/500] {'loss': Tensor(3523.36057959), 'test_loss': 3689.2049847299663}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 416/500] {'loss': Tensor(3521.72889235), 'test_loss': 3687.083557824504}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 417/500] {'loss': Tensor(3520.10353725), 'test_loss': 3684.9702023405684}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 418/500] {'loss': Tensor(3518.48447851), 'test_loss': 3682.864876596029}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 419/500] {'loss': Tensor(3516.87168058), 'test_loss': 3680.767539167357}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 420/500] {'loss': Tensor(3515.26510817), 'test_loss': 3678.6781488878787}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 421/500] {'loss': Tensor(3513.66472622), 'test_loss': 3676.5966648460317}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 422/500] {'loss': Tensor(3512.07049992), 'test_loss': 3674.5230463836338}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 423/500] {'loss': Tensor(3510.48239467), 'test_loss': 3672.45725309417}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 424/500] {'loss': Tensor(3508.90037613), 'test_loss': 3670.3992448210934}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 425/500] {'loss': Tensor(3507.32441017), 'test_loss': 3668.348981656121}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 426/500] {'loss': Tensor(3505.75446291), 'test_loss': 3666.3064239375694}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 427/500] {'loss': Tensor(3504.19050068), 'test_loss': 3664.2715322486742}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 428/500] {'loss': Tensor(3502.63249005), 'test_loss': 3662.244267415939}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 429/500] {'loss': Tensor(3501.08039781), 'test_loss': 3660.224590507491}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 430/500] {'loss': Tensor(3499.53419096), 'test_loss': 3658.2124628314473}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 431/500] {'loss': Tensor(3497.99383676), 'test_loss': 3656.2078459342947}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 432/500] {'loss': Tensor(3496.45930265), 'test_loss': 3654.2107015992756}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 433/500] {'loss': Tensor(3494.93055631), 'test_loss': 3652.2209918447948}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 434/500] {'loss': Tensor(3493.40756563), 'test_loss': 3650.2386789228285}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 435/500] {'loss': Tensor(3491.89029872), 'test_loss': 3648.263725317349}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 436/500] {'loss': Tensor(3490.3787239), 'test_loss': 3646.2960937427597}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 437/500] {'loss': Tensor(3488.87280971), 'test_loss': 3644.335747142342}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 438/500] {'loss': Tensor(3487.37252489), 'test_loss': 3642.382648686711}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 439/500] {'loss': Tensor(3485.87783839), 'test_loss': 3640.4367617722824}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 440/500] {'loss': Tensor(3484.38871939), 'test_loss': 3638.498050019752}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 441/500] {'loss': Tensor(3482.90513725), 'test_loss': 3636.5664772725863}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 442/500] {'loss': Tensor(3481.42706154), 'test_loss': 3634.64200759552}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 443/500] {'loss': Tensor(3479.95446205), 'test_loss': 3632.7246052730675}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 444/500] {'loss': Tensor(3478.48730875), 'test_loss': 3630.8142348080455}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 445/500] {'loss': Tensor(3477.02557183), 'test_loss': 3628.9108609200994}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 446/500] {'loss': Tensor(3475.56922167), 'test_loss': 3627.014448544248}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 447/500] {'loss': Tensor(3474.11822884), 'test_loss': 3625.1249628294418}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 448/500] {'loss': Tensor(3472.67256412), 'test_loss': 3623.2423691371105}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 449/500] {'loss': Tensor(3471.23219849), 'test_loss': 3621.3666330397496}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 450/500] {'loss': Tensor(3469.7971031), 'test_loss': 3619.497720319492}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 451/500] {'loss': Tensor(3468.36724932), 'test_loss': 3617.635596966707}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 452/500] {'loss': Tensor(3466.94260868), 'test_loss': 3615.780229178601}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 453/500] {'loss': Tensor(3465.52315293), 'test_loss': 3613.9315833578253}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 454/500] {'loss': Tensor(3464.10885399), 'test_loss': 3612.089626111104}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 455/500] {'loss': Tensor(3462.69968397), 'test_loss': 3610.2543242478623}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 456/500] {'loss': Tensor(3461.29561517), 'test_loss': 3608.4256447788644}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 457/500] {'loss': Tensor(3459.89662007), 'test_loss': 3606.6035549148696}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 458/500] {'loss': Tensor(3458.50267134), 'test_loss': 3604.788022065289}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 459/500] {'loss': Tensor(3457.11374181), 'test_loss': 3602.9790138368558}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 460/500] {'loss': Tensor(3455.72980452), 'test_loss': 3601.1764980323046}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 461/500] {'loss': Tensor(3454.35083267), 'test_loss': 3599.3804426490556}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 462/500] {'loss': Tensor(3452.97679965), 'test_loss': 3597.590815877918}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 463/500] {'loss': Tensor(3451.607679), 'test_loss': 3595.8075861017924}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 464/500] {'loss': Tensor(3450.24344447), 'test_loss': 3594.0307218943863}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 465/500] {'loss': Tensor(3448.88406997), 'test_loss': 3592.2601920189422}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 466/500] {'loss': Tensor(3447.52952956), 'test_loss': 3590.495965426962}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 467/500] {'loss': Tensor(3446.17979752), 'test_loss': 3588.7380112569604}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 468/500] {'loss': Tensor(3444.83484825), 'test_loss': 3586.9862988332093}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 469/500] {'loss': Tensor(3443.49465636), 'test_loss': 3585.240797664498}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 470/500] {'loss': Tensor(3442.15919659), 'test_loss': 3583.5014774429046}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 471/500] {'loss': Tensor(3440.82844388), 'test_loss': 3581.76830804257}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 472/500] {'loss': Tensor(3439.50237331), 'test_loss': 3580.0412595184885}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 473/500] {'loss': Tensor(3438.18096014), 'test_loss': 3578.3203021052927}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 474/500] {'loss': Tensor(3436.86417979), 'test_loss': 3576.6054062160665}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 475/500] {'loss': Tensor(3435.55200784), 'test_loss': 3574.896542441148}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 476/500] {'loss': Tensor(3434.24442004), 'test_loss': 3573.1936815469585}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 477/500] {'loss': Tensor(3432.94139227), 'test_loss': 3571.496794474818}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 478/500] {'loss': Tensor(3431.6429006), 'test_loss': 3569.80585233979}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 479/500] {'loss': Tensor(3430.34892125), 'test_loss': 3568.120826429522}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 480/500] {'loss': Tensor(3429.05943059), 'test_loss': 3566.441688203098}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 481/500] {'loss': Tensor(3427.77440514), 'test_loss': 3564.7684092899012}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 482/500] {'loss': Tensor(3426.49382159), 'test_loss': 3563.1009614884756}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 483/500] {'loss': Tensor(3425.21765677), 'test_loss': 3561.4393167654084}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 484/500] {'loss': Tensor(3423.94588768), 'test_loss': 3559.783447254213}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 485/500] {'loss': Tensor(3422.67849144), 'test_loss': 3558.1333252542163}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 486/500] {'loss': Tensor(3421.41544534), 'test_loss': 3556.488923229462}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 487/500] {'loss': Tensor(3420.15672682), 'test_loss': 3554.8502138076183}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 488/500] {'loss': Tensor(3418.90231346), 'test_loss': 3553.2171697788876}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 489/500] {'loss': Tensor(3417.65218299), 'test_loss': 3551.589764094935}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 490/500] {'loss': Tensor(3416.40631329), 'test_loss': 3549.9679698678146}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 491/500] {'loss': Tensor(3415.16468237), 'test_loss': 3548.3517603689065}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 492/500] {'loss': Tensor(3413.9272684), 'test_loss': 3546.7411090278642}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 493/500] {'loss': Tensor(3412.69404968), 'test_loss': 3545.1359894315624}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 494/500] {'loss': Tensor(3411.46500466), 'test_loss': 3543.5363753230627}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 495/500] {'loss': Tensor(3410.24011192), 'test_loss': 3541.942240600573}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 496/500] {'loss': Tensor(3409.0193502), 'test_loss': 3540.3535593164265}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 497/500] {'loss': Tensor(3407.80269836), 'test_loss': 3538.77030567606}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 498/500] {'loss': Tensor(3406.59013539), 'test_loss': 3537.1924540370055}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 499/500] {'loss': Tensor(3405.38164044), 'test_loss': 3535.619978907882}\n",
            "(353, 10)\n",
            "(353, 1)\n",
            "(89, 10)\n",
            "(89, 1)\n",
            "[Epoch 500/500] {'loss': Tensor(3404.17719278), 'test_loss': 3534.052854947397}\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class FakeTorchModule:\n",
        "    \"\"\"\n",
        "        Needed so that we can do manual linear regression.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.device = \"\"\n",
        "\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        return self.forward(*args, **kwargs)\n",
        "\n",
        "    def to(self, device):\n",
        "        return self\n",
        "\n",
        "    def parameters(self):\n",
        "        params = []\n",
        "        for k,v in self.__dict__.items():\n",
        "            params += getattr(v, 'parameters', lambda: [])()\n",
        "        return params\n",
        "\n",
        "    def train(self):\n",
        "        for p in self.parameters():\n",
        "            p.requires_grad = getattr(p, 'required_grad', p.requires_grad)\n",
        "    \n",
        "    def eval(self):\n",
        "        for p in self.parameters():\n",
        "            p.required_grad = p.requires_grad\n",
        "            p.requires_grad = False\n",
        "\n",
        "class ManualRegression(FakeTorchModule):\n",
        "    \"\"\"\n",
        "        Allows us to use our custom Linear layer and SGD optimizer.\n",
        "        Subclasses FakeTorchModule\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dims, output_dims):\n",
        "        super().__init__()\n",
        "        ## TODO: Incorporate your custom components in the initialization pipeline. \n",
        "        self.optimizer = None\n",
        "        self.dense = Linear(input_dims, output_dims)\n",
        "        self.loss = None\n",
        "        self.lr = None\n",
        "        self.activation = None\n",
        "        #self.weights, self.bias = self.dense.parameters()\n",
        "        self.set_learning_rate() \n",
        "\n",
        "    def set_learning_rate(self, learning_rate=0.001):\n",
        "        ## TODO: Use your new SGD component and make changes as appropriate.\n",
        "        self.lr = learning_rate\n",
        "        self.optimizer = SGD(self.dense.parameters(), lr=learning_rate) \n",
        "\n",
        "    def forward(self, x):\n",
        "        ## TODO: Implement the forward function as appropriate. Make changes as necessary \n",
        "        print(x.shape)\n",
        "        x = self.dense(x) \n",
        "        print(x.shape)\n",
        "        x = self.activation(x)\n",
        "        return x\n",
        "\n",
        "class TrainTest2(TrainTest):\n",
        "    # no_grad = torch.no_grad\n",
        "    no_grad = Tensor.no_grad\n",
        "\n",
        "class ManualLinearRegression(ManualRegression, TrainTest2):\n",
        "    def __init__(self, input_dims, output_dims):\n",
        "        super().__init__(input_dims, output_dims)\n",
        "        ## TODO: Implement the subclass as appropriate with your own implementations\n",
        "        self.loss = MSELoss()\n",
        "        self.activation = nn.Identity()\n",
        "        # self.model = ManualRegression(input_dims, output_dims)\n",
        "\n",
        "\n",
        "## Train the manual linear regression model\n",
        "model = ManualLinearRegression(10, 1)\n",
        "model.set_learning_rate(0.2)\n",
        "model.train_test(\n",
        "    [[Tensor(X0), Tensor(Y0)]], \n",
        "    [[Tensor(X1), Tensor(Y1)]],\n",
        "    epochs=500\n",
        ");\n",
        "## TODO: Compare this model's performance to the first linear regression model -- they should be similar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhxMFrlPOX2J"
      },
      "source": [
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6DGrd50k0J5"
      },
      "source": [
        "## Wrapping Up\n",
        "\n",
        "Congratulations, you've finished this assignment! You should now have a better understand of linear regression, loss functions, and optimizers/gradient descent. This assignment provides the foundation for Homework 2, so feel free to come back or read it over again to get a solid understanding.\n",
        "\n",
        "Be sure to submit your finished notebook (follow the guidelines on the handout)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "759be6693a164ddeab1e231298c2a01a8302a7c7dfd4e560844dbce42a896f34"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}